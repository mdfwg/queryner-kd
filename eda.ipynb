{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a33251",
   "metadata": {},
   "source": [
    "## Distribution Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from collections import defaultdict\n",
    "# import random\n",
    "\n",
    "# # Terapkan random seed untuk hasil yang dapat direproduksi\n",
    "# random.seed(42)\n",
    "\n",
    "# # Muat dataset\n",
    "# dataset = load_dataset(\"bltlab/queryner\")\n",
    "# label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "# # --- 1. Konsolidasi Label BIO ---\n",
    "# # Fungsi untuk menggabungkan label B- dan I- menjadi satu entitas\n",
    "# consolidated_labels_map = {}\n",
    "# consolidated_label_names = []\n",
    "# for label in label_list:\n",
    "#     if label.startswith('B-'):\n",
    "#         entity_name = label[2:]\n",
    "#         consolidated_labels_map[label] = entity_name\n",
    "#         consolidated_label_names.append(entity_name)\n",
    "#     elif label.startswith('I-'):\n",
    "#         entity_name = label[2:]\n",
    "#         consolidated_labels_map[label] = entity_name\n",
    "#         consolidated_label_names.append(entity_name)\n",
    "#     else: # 'O' label\n",
    "#         consolidated_labels_map[label] = label\n",
    "#         consolidated_label_names.append(label)\n",
    "# consolidated_label_names = sorted(list(set(consolidated_label_names)))\n",
    "\n",
    "# # Daftar label yang terpengaruh\n",
    "# labels_to_plot = ['condition', 'quantity', 'price', 'origin', 'time', 'product_number']\n",
    "\n",
    "# # Fungsi untuk menghitung entitas di split dan mengkonsolidasikan\n",
    "# def count_and_consolidate(split_data):\n",
    "#     counts = defaultdict(int)\n",
    "#     for example in split_data:\n",
    "#         for tag_id in example['ner_tags']:\n",
    "#             original_label = label_list[tag_id]\n",
    "#             consolidated_label = consolidated_labels_map[original_label]\n",
    "#             counts[consolidated_label] += 1\n",
    "#     return counts\n",
    "\n",
    "# # Fungsi untuk menghitung data dan persentase\n",
    "# def calculate_distribution(counts, labels_to_check):\n",
    "#     plot_data = defaultdict(dict)\n",
    "#     total_counts = {}\n",
    "\n",
    "#     for label in labels_to_check:\n",
    "#         total_counts[label] = sum(counts[split][label] for split in ['train', 'validation', 'test'])\n",
    "\n",
    "#     for label in labels_to_check:\n",
    "#         total = total_counts[label]\n",
    "#         if total > 0:\n",
    "#             for split in ['train', 'validation', 'test']:\n",
    "#                 count = counts[split][label]\n",
    "#                 percentage = (count / total) * 100 if total > 0 else 0\n",
    "#                 plot_data[label][split] = {'percentage': round(percentage, 2), 'count': count}\n",
    "#         else:\n",
    "#             for split in ['train', 'validation', 'test']:\n",
    "#                 plot_data[label][split] = {'percentage': 0, 'count': 0}\n",
    "\n",
    "#     return plot_data\n",
    "\n",
    "# # --- Tahap Awal: Hitung Distribusi Sebelum Penyesuaian ---\n",
    "# print(\"Menghitung distribusi entitas sebelum penyesuaian...\")\n",
    "# initial_counts = {\n",
    "#     'train': count_and_consolidate(dataset['train']),\n",
    "#     'validation': count_and_consolidate(dataset['validation']),\n",
    "#     'test': count_and_consolidate(dataset['test'])\n",
    "# }\n",
    "# initial_distribution = calculate_distribution(initial_counts, labels_to_plot)\n",
    "\n",
    "# # --- Tahap Penyesuaian Distribusi Entitas ---\n",
    "# print(\"\\nMelakukan penyesuaian distribusi entitas...\")\n",
    "# rebalanced_dataset = {split: list(dataset[split]) for split in dataset.keys()}\n",
    "\n",
    "# def move_samples(source_split_name, dest_split_name, label_to_move, num_to_move):\n",
    "#     global rebalanced_dataset\n",
    "    \n",
    "#     source_split = rebalanced_dataset[source_split_name]\n",
    "#     dest_split = rebalanced_dataset[dest_split_name]\n",
    "    \n",
    "#     potential_indices = [i for i, example in enumerate(source_split) if any(label_to_move in label_list[tag_id] for tag_id in example['ner_tags'])]\n",
    "    \n",
    "#     if len(potential_indices) >= num_to_move:\n",
    "#         indices_to_remove = random.sample(potential_indices, num_to_move)\n",
    "#     else:\n",
    "#         indices_to_remove = potential_indices\n",
    "    \n",
    "#     if len(indices_to_remove) > 0:\n",
    "#         samples_to_move = [source_split[i] for i in indices_to_remove]\n",
    "#         for i in sorted(indices_to_remove, reverse=True):\n",
    "#             del source_split[i]\n",
    "#         dest_split.extend(samples_to_move)\n",
    "\n",
    "# # Terapkan penyesuaian yang dijelaskan:\n",
    "# move_samples('train', 'test', 'condition', 15)\n",
    "# move_samples('test', 'train', 'quantity', 5)\n",
    "# move_samples('train', 'validation', 'price', 3)\n",
    "# move_samples('train', 'test', 'price', 2)\n",
    "# move_samples('test', 'validation', 'origin', 3)\n",
    "# move_samples('train', 'test', 'time', 3)\n",
    "# move_samples('test', 'validation', 'product_number', 3)\n",
    "\n",
    "# # --- Tahap Akhir: Hitung ulang distribusi setelah penyesuaian ---\n",
    "# print(\"\\nPenyesuaian selesai. Menghitung ulang distribusi entitas...\")\n",
    "# final_counts = {\n",
    "#     'train': count_and_consolidate(rebalanced_dataset['train']),\n",
    "#     'validation': count_and_consolidate(rebalanced_dataset['validation']),\n",
    "#     'test': count_and_consolidate(rebalanced_dataset['test'])\n",
    "# }\n",
    "# final_distribution = calculate_distribution(final_counts, labels_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# os.makedirs('data/raw', exist_ok=True)\n",
    "# os.makedirs('data/processed', exist_ok=True)\n",
    "# os.makedirs('data/distribution', exist_ok=True)\n",
    "\n",
    "# # Save original dataset to raw folder\n",
    "# print(\"\\nSaving original dataset...\")\n",
    "# for split in dataset.keys():\n",
    "#     dataset[split].to_json(f'data/raw/{split}.json')\n",
    "# print(\"Original dataset saved to data/raw/\")\n",
    "\n",
    "# # Save rebalanced dataset to processed folder\n",
    "# print(\"\\nSaving processed dataset...\")\n",
    "# for split, data in rebalanced_dataset.items():\n",
    "#     # Convert list of examples to a format that can be saved\n",
    "#     processed_data = {\n",
    "#         'examples': data\n",
    "#     }\n",
    "#     with open(f'data/processed/{split}.json', 'w', encoding='utf-8') as f:\n",
    "#         json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "# print(\"Processed dataset saved to data/processed/\")\n",
    "\n",
    "# # Save distributions to distribution folder\n",
    "# print(\"\\nSaving distribution data...\")\n",
    "# distribution_data = {\n",
    "#     'initial_distribution': initial_distribution,\n",
    "#     'final_distribution': final_distribution\n",
    "# }\n",
    "# with open('data/distribution/label_distributions.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(distribution_data, f, ensure_ascii=False, indent=2)\n",
    "# print(\"Distribution data saved to data/distribution/label_distributions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf465a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reformatted D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\n",
      "  Total examples: 7841\n",
      "✓ Reformatted D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\n",
      "  Total examples: 871\n",
      "✓ Reformatted D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\n",
      "  Total examples: 993\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# def reformat_jsonl_file(file_path):\n",
    "#     examples = []\n",
    "\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             if line.strip():  # Skip empty lines\n",
    "#                 data = json.loads(line)\n",
    "#                 examples.append(data)\n",
    "    \n",
    "#     # Create output structure\n",
    "#     output_data = {\"examples\": examples}\n",
    "    \n",
    "#     # Overwrite the original file\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "#     print(f\"✓ Reformatted {file_path}\")\n",
    "#     print(f\"  Total examples: {len(examples)}\")\n",
    "\n",
    "\n",
    "# # Usage examples:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Single file\n",
    "#     reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\")\n",
    "#     reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\")\n",
    "#     reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\")\n",
    "    \n",
    "#     # Or batch process all files\n",
    "#     # raw_dir = Path(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\")\n",
    "#     # for json_file in raw_dir.glob(\"*.json\"):\n",
    "#     #     reformat_jsonl_file(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd34d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\train.json\", 'r', encoding='utf-8') as f:\n",
    "#     print(len(json.load(f)['examples']))\n",
    "# with open(r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\validation.json\", 'r', encoding='utf-8') as f:\n",
    "#     print(len(json.load(f)['examples']))\n",
    "# with open(r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\test.json\", 'r', encoding='utf-8') as f:\n",
    "#     print(len(json.load(f)['examples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a92d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARING TRAIN\n",
      "================================================================================\n",
      "Raw: 7841 examples\n",
      "Processed: 7823 examples\n",
      "Difference: 18 examples\n",
      "\n",
      "Only in RAW (removed): 23 examples\n",
      "Only in PROCESSED (added): 5 examples\n",
      "\n",
      "First 5 examples REMOVED from raw -> processed:\n",
      "\n",
      "1. Tokens: ('rustic', 'grey', 'wall', 'mirror')\n",
      "   Tags: (5, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('director', 'chair', 'without', 'arms')\n",
      "   Tags: (17, 9, 5, 6)\n",
      "\n",
      "3. Tokens: ('good', 'drawing', 'tablets', 'with', 'screens')\n",
      "   Tags: (5, 9, 10, 17, 18)\n",
      "\n",
      "4. Tokens: ('10m', 'waterproof', 'led', 'strip', 'lights', 'outdoor', 'without', 'controller')\n",
      "   Tags: (17, 18, 9, 10, 10, 17, 5, 6)\n",
      "\n",
      "5. Tokens: ('03', 'dodge', 'ram', 'visor')\n",
      "   Tags: (33, 11, 9, 10)\n",
      "\n",
      "First 5 examples ADDED in processed:\n",
      "\n",
      "1. Tokens: ('bulk', 'tote', 'bags')\n",
      "   Tags: (29, 9, 10)\n",
      "\n",
      "2. Tokens: ('2', 'piece', 'swimsuits', 'for', 'women')\n",
      "   Tags: (29, 30, 9, 17, 13)\n",
      "\n",
      "3. Tokens: ('1', 'huba', 'buba', 'origanal', 'not', 'exspencive')\n",
      "   Tags: (29, 25, 26, 5, 23, 24)\n",
      "\n",
      "4. Tokens: ('dog', 'food', '100', 'lb', 'bag')\n",
      "   Tags: (9, 10, 29, 30, 30)\n",
      "\n",
      "5. Tokens: ('push', 'pop', 'variety', 'pack')\n",
      "   Tags: (25, 26, 29, 30)\n",
      "\n",
      "✓ Detailed report saved to: results\\data_analysis\\diff_train.json\n",
      "\n",
      "================================================================================\n",
      "COMPARING VALIDATION\n",
      "================================================================================\n",
      "Raw: 871 examples\n",
      "Processed: 880 examples\n",
      "Difference: -9 examples\n",
      "\n",
      "Only in RAW (removed): 0 examples\n",
      "Only in PROCESSED (added): 9 examples\n",
      "\n",
      "First 5 examples ADDED in processed:\n",
      "\n",
      "1. Tokens: ('cannon', '243', 'black', 'ink', 'cartridges')\n",
      "   Tags: (11, 27, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('tablets', 'on', 'sale', 'prime')\n",
      "   Tags: (9, 23, 24, 0)\n",
      "\n",
      "3. Tokens: ('intel', 'nuc10i7fnk')\n",
      "   Tags: (11, 27)\n",
      "\n",
      "4. Tokens: ('hero', '8', 'black')\n",
      "   Tags: (27, 28, 3)\n",
      "\n",
      "5. Tokens: ('russian', 'military', 'boots')\n",
      "   Tags: (21, 17, 9)\n",
      "\n",
      "✓ Detailed report saved to: results\\data_analysis\\diff_validation.json\n",
      "\n",
      "================================================================================\n",
      "COMPARING TEST\n",
      "================================================================================\n",
      "Raw: 993 examples\n",
      "Processed: 1002 examples\n",
      "Difference: -9 examples\n",
      "\n",
      "Only in RAW (removed): 11 examples\n",
      "Only in PROCESSED (added): 20 examples\n",
      "\n",
      "First 5 examples REMOVED from raw -> processed:\n",
      "\n",
      "1. Tokens: ('cannon', '243', 'black', 'ink', 'cartridges')\n",
      "   Tags: (11, 27, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('bulk', 'tote', 'bags')\n",
      "   Tags: (29, 9, 10)\n",
      "\n",
      "3. Tokens: ('intel', 'nuc10i7fnk')\n",
      "   Tags: (11, 27)\n",
      "\n",
      "4. Tokens: ('hero', '8', 'black')\n",
      "   Tags: (27, 28, 3)\n",
      "\n",
      "5. Tokens: ('russian', 'military', 'boots')\n",
      "   Tags: (21, 17, 9)\n",
      "\n",
      "First 5 examples ADDED in processed:\n",
      "\n",
      "1. Tokens: ('rustic', 'grey', 'wall', 'mirror')\n",
      "   Tags: (5, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('director', 'chair', 'without', 'arms')\n",
      "   Tags: (17, 9, 5, 6)\n",
      "\n",
      "3. Tokens: ('good', 'drawing', 'tablets', 'with', 'screens')\n",
      "   Tags: (5, 9, 10, 17, 18)\n",
      "\n",
      "4. Tokens: ('10m', 'waterproof', 'led', 'strip', 'lights', 'outdoor', 'without', 'controller')\n",
      "   Tags: (17, 18, 9, 10, 10, 17, 5, 6)\n",
      "\n",
      "5. Tokens: ('03', 'dodge', 'ram', 'visor')\n",
      "   Tags: (33, 11, 9, 10)\n",
      "\n",
      "✓ Detailed report saved to: results\\data_analysis\\diff_test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def compare_datasets(raw_dir, processed_dir):\n",
    "    \"\"\"\n",
    "    Compare raw and processed datasets and find differences.\n",
    "    \n",
    "    Args:\n",
    "        raw_dir: Path to raw data directory\n",
    "        processed_dir: Path to processed data directory\n",
    "    \"\"\"\n",
    "    \n",
    "    splits = [\"train\", \"validation\", \"test\"]\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COMPARING {split.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        raw_path = Path(raw_dir) / f\"{split}.json\"\n",
    "        with open(raw_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)['examples']\n",
    "        \n",
    "        # Load processed data\n",
    "        processed_path = Path(processed_dir) / f\"{split}.json\"\n",
    "        with open(processed_path, 'r', encoding='utf-8') as f:\n",
    "            processed_data = json.load(f)['examples']\n",
    "        \n",
    "        print(f\"Raw: {len(raw_data)} examples\")\n",
    "        print(f\"Processed: {len(processed_data)} examples\")\n",
    "        print(f\"Difference: {len(raw_data) - len(processed_data)} examples\\n\")\n",
    "        \n",
    "        # Find differences\n",
    "        raw_set = set()\n",
    "        processed_set = set()\n",
    "        \n",
    "        # Convert to hashable format (tuple of tokens and tags)\n",
    "        for example in raw_data:\n",
    "            key = (tuple(example['tokens']), tuple(example['ner_tags']))\n",
    "            raw_set.add(key)\n",
    "        \n",
    "        for example in processed_data:\n",
    "            key = (tuple(example['tokens']), tuple(example['ner_tags']))\n",
    "            processed_set.add(key)\n",
    "        \n",
    "        # Find differences\n",
    "        only_in_raw = raw_set - processed_set\n",
    "        only_in_processed = processed_set - raw_set\n",
    "        \n",
    "        print(f\"Only in RAW (removed): {len(only_in_raw)} examples\")\n",
    "        print(f\"Only in PROCESSED (added): {len(only_in_processed)} examples\")\n",
    "        \n",
    "        # Display first few differences\n",
    "        if only_in_raw:\n",
    "            print(f\"\\nFirst 5 examples REMOVED from raw -> processed:\")\n",
    "            for i, (tokens, tags) in enumerate(list(only_in_raw)[:5], 1):\n",
    "                print(f\"\\n{i}. Tokens: {tokens}\")\n",
    "                print(f\"   Tags: {tags}\")\n",
    "        \n",
    "        if only_in_processed:\n",
    "            print(f\"\\nFirst 5 examples ADDED in processed:\")\n",
    "            for i, (tokens, tags) in enumerate(list(only_in_processed)[:5], 1):\n",
    "                print(f\"\\n{i}. Tokens: {tokens}\")\n",
    "                print(f\"   Tags: {tags}\")\n",
    "        \n",
    "        # Save detailed diff report\n",
    "        save_diff_report(split, only_in_raw, only_in_processed)\n",
    "\n",
    "\n",
    "def save_diff_report(split, only_in_raw, only_in_processed):\n",
    "    \"\"\"Save detailed diff report to file\"\"\"\n",
    "    \n",
    "    output_dir = Path(\"results/data_analysis\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    report_path = output_dir / f\"diff_{split}.json\"\n",
    "    \n",
    "    report = {\n",
    "        \"split\": split,\n",
    "        \"only_in_raw\": [\n",
    "            {\n",
    "                \"tokens\": list(tokens),\n",
    "                \"ner_tags\": list(tags)\n",
    "            }\n",
    "            for tokens, tags in only_in_raw\n",
    "        ],\n",
    "        \"only_in_processed\": [\n",
    "            {\n",
    "                \"tokens\": list(tokens),\n",
    "                \"ner_tags\": list(tags)\n",
    "            }\n",
    "            for tokens, tags in only_in_processed\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✓ Detailed report saved to: {report_path}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    raw_dir = r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\"\n",
    "    processed_dir = r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\"\n",
    "    \n",
    "    compare_datasets(raw_dir, processed_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
