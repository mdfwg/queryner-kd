{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a33251",
   "metadata": {},
   "source": [
    "## Distribution Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from collections import defaultdict\n",
    "# import random\n",
    "\n",
    "# # Terapkan random seed untuk hasil yang dapat direproduksi\n",
    "# random.seed(42)\n",
    "\n",
    "# # Muat dataset\n",
    "# dataset = load_dataset(\"bltlab/queryner\")\n",
    "# label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "# # --- 1. Konsolidasi Label BIO ---\n",
    "# # Fungsi untuk menggabungkan label B- dan I- menjadi satu entitas\n",
    "# consolidated_labels_map = {}\n",
    "# consolidated_label_names = []\n",
    "# for label in label_list:\n",
    "#     if label.startswith('B-'):\n",
    "#         entity_name = label[2:]\n",
    "#         consolidated_labels_map[label] = entity_name\n",
    "#         consolidated_label_names.append(entity_name)\n",
    "#     elif label.startswith('I-'):\n",
    "#         entity_name = label[2:]\n",
    "#         consolidated_labels_map[label] = entity_name\n",
    "#         consolidated_label_names.append(entity_name)\n",
    "#     else: # 'O' label\n",
    "#         consolidated_labels_map[label] = label\n",
    "#         consolidated_label_names.append(label)\n",
    "# consolidated_label_names = sorted(list(set(consolidated_label_names)))\n",
    "\n",
    "# # Daftar label yang terpengaruh\n",
    "# labels_to_plot = ['condition', 'quantity', 'price', 'origin', 'time', 'product_number']\n",
    "\n",
    "# # Fungsi untuk menghitung entitas di split dan mengkonsolidasikan\n",
    "# def count_and_consolidate(split_data):\n",
    "#     counts = defaultdict(int)\n",
    "#     for example in split_data:\n",
    "#         for tag_id in example['ner_tags']:\n",
    "#             original_label = label_list[tag_id]\n",
    "#             consolidated_label = consolidated_labels_map[original_label]\n",
    "#             counts[consolidated_label] += 1\n",
    "#     return counts\n",
    "\n",
    "# # Fungsi untuk menghitung data dan persentase\n",
    "# def calculate_distribution(counts, labels_to_check):\n",
    "#     plot_data = defaultdict(dict)\n",
    "#     total_counts = {}\n",
    "\n",
    "#     for label in labels_to_check:\n",
    "#         total_counts[label] = sum(counts[split][label] for split in ['train', 'validation', 'test'])\n",
    "\n",
    "#     for label in labels_to_check:\n",
    "#         total = total_counts[label]\n",
    "#         if total > 0:\n",
    "#             for split in ['train', 'validation', 'test']:\n",
    "#                 count = counts[split][label]\n",
    "#                 percentage = (count / total) * 100 if total > 0 else 0\n",
    "#                 plot_data[label][split] = {'percentage': round(percentage, 2), 'count': count}\n",
    "#         else:\n",
    "#             for split in ['train', 'validation', 'test']:\n",
    "#                 plot_data[label][split] = {'percentage': 0, 'count': 0}\n",
    "\n",
    "#     return plot_data\n",
    "\n",
    "# # --- Tahap Awal: Hitung Distribusi Sebelum Penyesuaian ---\n",
    "# print(\"Menghitung distribusi entitas sebelum penyesuaian...\")\n",
    "# initial_counts = {\n",
    "#     'train': count_and_consolidate(dataset['train']),\n",
    "#     'validation': count_and_consolidate(dataset['validation']),\n",
    "#     'test': count_and_consolidate(dataset['test'])\n",
    "# }\n",
    "# initial_distribution = calculate_distribution(initial_counts, labels_to_plot)\n",
    "\n",
    "# # --- Tahap Penyesuaian Distribusi Entitas ---\n",
    "# print(\"\\nMelakukan penyesuaian distribusi entitas...\")\n",
    "# rebalanced_dataset = {split: list(dataset[split]) for split in dataset.keys()}\n",
    "\n",
    "# def move_samples(source_split_name, dest_split_name, label_to_move, num_to_move):\n",
    "#     global rebalanced_dataset\n",
    "    \n",
    "#     source_split = rebalanced_dataset[source_split_name]\n",
    "#     dest_split = rebalanced_dataset[dest_split_name]\n",
    "    \n",
    "#     potential_indices = [i for i, example in enumerate(source_split) if any(label_to_move in label_list[tag_id] for tag_id in example['ner_tags'])]\n",
    "    \n",
    "#     if len(potential_indices) >= num_to_move:\n",
    "#         indices_to_remove = random.sample(potential_indices, num_to_move)\n",
    "#     else:\n",
    "#         indices_to_remove = potential_indices\n",
    "    \n",
    "#     if len(indices_to_remove) > 0:\n",
    "#         samples_to_move = [source_split[i] for i in indices_to_remove]\n",
    "#         for i in sorted(indices_to_remove, reverse=True):\n",
    "#             del source_split[i]\n",
    "#         dest_split.extend(samples_to_move)\n",
    "\n",
    "# # Terapkan penyesuaian yang dijelaskan:\n",
    "# move_samples('train', 'test', 'condition', 15)\n",
    "# move_samples('test', 'train', 'quantity', 5)\n",
    "# move_samples('train', 'validation', 'price', 3)\n",
    "# move_samples('train', 'test', 'price', 2)\n",
    "# move_samples('test', 'validation', 'origin', 3)\n",
    "# move_samples('train', 'test', 'time', 3)\n",
    "# move_samples('test', 'validation', 'product_number', 3)\n",
    "\n",
    "# # --- Tahap Akhir: Hitung ulang distribusi setelah penyesuaian ---\n",
    "# print(\"\\nPenyesuaian selesai. Menghitung ulang distribusi entitas...\")\n",
    "# final_counts = {\n",
    "#     'train': count_and_consolidate(rebalanced_dataset['train']),\n",
    "#     'validation': count_and_consolidate(rebalanced_dataset['validation']),\n",
    "#     'test': count_and_consolidate(rebalanced_dataset['test'])\n",
    "# }\n",
    "# final_distribution = calculate_distribution(final_counts, labels_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# os.makedirs('data/raw', exist_ok=True)\n",
    "# os.makedirs('data/processed', exist_ok=True)\n",
    "# os.makedirs('data/distribution', exist_ok=True)\n",
    "\n",
    "# # Save original dataset to raw folder\n",
    "# print(\"\\nSaving original dataset...\")\n",
    "# for split in dataset.keys():\n",
    "#     dataset[split].to_json(f'data/raw/{split}.json')\n",
    "# print(\"Original dataset saved to data/raw/\")\n",
    "\n",
    "# # Save rebalanced dataset to processed folder\n",
    "# print(\"\\nSaving processed dataset...\")\n",
    "# for split, data in rebalanced_dataset.items():\n",
    "#     # Convert list of examples to a format that can be saved\n",
    "#     processed_data = {\n",
    "#         'examples': data\n",
    "#     }\n",
    "#     with open(f'data/processed/{split}.json', 'w', encoding='utf-8') as f:\n",
    "#         json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "# print(\"Processed dataset saved to data/processed/\")\n",
    "\n",
    "# # Save distributions to distribution folder\n",
    "# print(\"\\nSaving distribution data...\")\n",
    "# distribution_data = {\n",
    "#     'initial_distribution': initial_distribution,\n",
    "#     'final_distribution': final_distribution\n",
    "# }\n",
    "# with open('data/distribution/label_distributions.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(distribution_data, f, ensure_ascii=False, indent=2)\n",
    "# print(\"Distribution data saved to data/distribution/label_distributions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf465a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reformatted D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\n",
      "  Total examples: 7841\n",
      "✓ Reformatted D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\n",
      "  Total examples: 871\n",
      "✓ Reformatted D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\n",
      "  Total examples: 993\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# def reformat_jsonl_file(file_path):\n",
    "#     examples = []\n",
    "\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             if line.strip():  # Skip empty lines\n",
    "#                 data = json.loads(line)\n",
    "#                 examples.append(data)\n",
    "    \n",
    "#     # Create output structure\n",
    "#     output_data = {\"examples\": examples}\n",
    "    \n",
    "#     # Overwrite the original file\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "#     print(f\"✓ Reformatted {file_path}\")\n",
    "#     print(f\"  Total examples: {len(examples)}\")\n",
    "\n",
    "\n",
    "# # Usage examples:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Single file\n",
    "#     reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\")\n",
    "#     reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\")\n",
    "#     reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\")\n",
    "    \n",
    "#     # Or batch process all files\n",
    "#     # raw_dir = Path(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\")\n",
    "#     # for json_file in raw_dir.glob(\"*.json\"):\n",
    "#     #     reformat_jsonl_file(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd34d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\train.json\", 'r', encoding='utf-8') as f:\n",
    "#     print(len(json.load(f)['examples']))\n",
    "# with open(r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\validation.json\", 'r', encoding='utf-8') as f:\n",
    "#     print(len(json.load(f)['examples']))\n",
    "# with open(r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\test.json\", 'r', encoding='utf-8') as f:\n",
    "#     print(len(json.load(f)['examples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a92d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARING TRAIN\n",
      "================================================================================\n",
      "Raw: 7841 examples\n",
      "Processed: 7823 examples\n",
      "Difference: 18 examples\n",
      "\n",
      "Only in RAW (removed): 23 examples\n",
      "Only in PROCESSED (added): 5 examples\n",
      "\n",
      "First 5 examples REMOVED from raw -> processed:\n",
      "\n",
      "1. Tokens: ('rustic', 'grey', 'wall', 'mirror')\n",
      "   Tags: (5, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('director', 'chair', 'without', 'arms')\n",
      "   Tags: (17, 9, 5, 6)\n",
      "\n",
      "3. Tokens: ('good', 'drawing', 'tablets', 'with', 'screens')\n",
      "   Tags: (5, 9, 10, 17, 18)\n",
      "\n",
      "4. Tokens: ('10m', 'waterproof', 'led', 'strip', 'lights', 'outdoor', 'without', 'controller')\n",
      "   Tags: (17, 18, 9, 10, 10, 17, 5, 6)\n",
      "\n",
      "5. Tokens: ('03', 'dodge', 'ram', 'visor')\n",
      "   Tags: (33, 11, 9, 10)\n",
      "\n",
      "First 5 examples ADDED in processed:\n",
      "\n",
      "1. Tokens: ('bulk', 'tote', 'bags')\n",
      "   Tags: (29, 9, 10)\n",
      "\n",
      "2. Tokens: ('2', 'piece', 'swimsuits', 'for', 'women')\n",
      "   Tags: (29, 30, 9, 17, 13)\n",
      "\n",
      "3. Tokens: ('1', 'huba', 'buba', 'origanal', 'not', 'exspencive')\n",
      "   Tags: (29, 25, 26, 5, 23, 24)\n",
      "\n",
      "4. Tokens: ('dog', 'food', '100', 'lb', 'bag')\n",
      "   Tags: (9, 10, 29, 30, 30)\n",
      "\n",
      "5. Tokens: ('push', 'pop', 'variety', 'pack')\n",
      "   Tags: (25, 26, 29, 30)\n",
      "\n",
      "✓ Detailed report saved to: results\\data_analysis\\diff_train.json\n",
      "\n",
      "================================================================================\n",
      "COMPARING VALIDATION\n",
      "================================================================================\n",
      "Raw: 871 examples\n",
      "Processed: 880 examples\n",
      "Difference: -9 examples\n",
      "\n",
      "Only in RAW (removed): 0 examples\n",
      "Only in PROCESSED (added): 9 examples\n",
      "\n",
      "First 5 examples ADDED in processed:\n",
      "\n",
      "1. Tokens: ('cannon', '243', 'black', 'ink', 'cartridges')\n",
      "   Tags: (11, 27, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('tablets', 'on', 'sale', 'prime')\n",
      "   Tags: (9, 23, 24, 0)\n",
      "\n",
      "3. Tokens: ('intel', 'nuc10i7fnk')\n",
      "   Tags: (11, 27)\n",
      "\n",
      "4. Tokens: ('hero', '8', 'black')\n",
      "   Tags: (27, 28, 3)\n",
      "\n",
      "5. Tokens: ('russian', 'military', 'boots')\n",
      "   Tags: (21, 17, 9)\n",
      "\n",
      "✓ Detailed report saved to: results\\data_analysis\\diff_validation.json\n",
      "\n",
      "================================================================================\n",
      "COMPARING TEST\n",
      "================================================================================\n",
      "Raw: 993 examples\n",
      "Processed: 1002 examples\n",
      "Difference: -9 examples\n",
      "\n",
      "Only in RAW (removed): 11 examples\n",
      "Only in PROCESSED (added): 20 examples\n",
      "\n",
      "First 5 examples REMOVED from raw -> processed:\n",
      "\n",
      "1. Tokens: ('cannon', '243', 'black', 'ink', 'cartridges')\n",
      "   Tags: (11, 27, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('bulk', 'tote', 'bags')\n",
      "   Tags: (29, 9, 10)\n",
      "\n",
      "3. Tokens: ('intel', 'nuc10i7fnk')\n",
      "   Tags: (11, 27)\n",
      "\n",
      "4. Tokens: ('hero', '8', 'black')\n",
      "   Tags: (27, 28, 3)\n",
      "\n",
      "5. Tokens: ('russian', 'military', 'boots')\n",
      "   Tags: (21, 17, 9)\n",
      "\n",
      "First 5 examples ADDED in processed:\n",
      "\n",
      "1. Tokens: ('rustic', 'grey', 'wall', 'mirror')\n",
      "   Tags: (5, 3, 9, 10)\n",
      "\n",
      "2. Tokens: ('director', 'chair', 'without', 'arms')\n",
      "   Tags: (17, 9, 5, 6)\n",
      "\n",
      "3. Tokens: ('good', 'drawing', 'tablets', 'with', 'screens')\n",
      "   Tags: (5, 9, 10, 17, 18)\n",
      "\n",
      "4. Tokens: ('10m', 'waterproof', 'led', 'strip', 'lights', 'outdoor', 'without', 'controller')\n",
      "   Tags: (17, 18, 9, 10, 10, 17, 5, 6)\n",
      "\n",
      "5. Tokens: ('03', 'dodge', 'ram', 'visor')\n",
      "   Tags: (33, 11, 9, 10)\n",
      "\n",
      "✓ Detailed report saved to: results\\data_analysis\\diff_test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def compare_datasets(raw_dir, processed_dir):\n",
    "    \"\"\"\n",
    "    Compare raw and processed datasets and find differences.\n",
    "    \n",
    "    Args:\n",
    "        raw_dir: Path to raw data directory\n",
    "        processed_dir: Path to processed data directory\n",
    "    \"\"\"\n",
    "    \n",
    "    splits = [\"train\", \"validation\", \"test\"]\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COMPARING {split.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        raw_path = Path(raw_dir) / f\"{split}.json\"\n",
    "        with open(raw_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)['examples']\n",
    "        \n",
    "        # Load processed data\n",
    "        processed_path = Path(processed_dir) / f\"{split}.json\"\n",
    "        with open(processed_path, 'r', encoding='utf-8') as f:\n",
    "            processed_data = json.load(f)['examples']\n",
    "        \n",
    "        print(f\"Raw: {len(raw_data)} examples\")\n",
    "        print(f\"Processed: {len(processed_data)} examples\")\n",
    "        print(f\"Difference: {len(raw_data) - len(processed_data)} examples\\n\")\n",
    "        \n",
    "        # Find differences\n",
    "        raw_set = set()\n",
    "        processed_set = set()\n",
    "        \n",
    "        # Convert to hashable format (tuple of tokens and tags)\n",
    "        for example in raw_data:\n",
    "            key = (tuple(example['tokens']), tuple(example['ner_tags']))\n",
    "            raw_set.add(key)\n",
    "        \n",
    "        for example in processed_data:\n",
    "            key = (tuple(example['tokens']), tuple(example['ner_tags']))\n",
    "            processed_set.add(key)\n",
    "        \n",
    "        # Find differences\n",
    "        only_in_raw = raw_set - processed_set\n",
    "        only_in_processed = processed_set - raw_set\n",
    "        \n",
    "        print(f\"Only in RAW (removed): {len(only_in_raw)} examples\")\n",
    "        print(f\"Only in PROCESSED (added): {len(only_in_processed)} examples\")\n",
    "        \n",
    "        # Display first few differences\n",
    "        if only_in_raw:\n",
    "            print(f\"\\nFirst 5 examples REMOVED from raw -> processed:\")\n",
    "            for i, (tokens, tags) in enumerate(list(only_in_raw)[:5], 1):\n",
    "                print(f\"\\n{i}. Tokens: {tokens}\")\n",
    "                print(f\"   Tags: {tags}\")\n",
    "        \n",
    "        if only_in_processed:\n",
    "            print(f\"\\nFirst 5 examples ADDED in processed:\")\n",
    "            for i, (tokens, tags) in enumerate(list(only_in_processed)[:5], 1):\n",
    "                print(f\"\\n{i}. Tokens: {tokens}\")\n",
    "                print(f\"   Tags: {tags}\")\n",
    "        \n",
    "        # Save detailed diff report\n",
    "        save_diff_report(split, only_in_raw, only_in_processed)\n",
    "\n",
    "\n",
    "def save_diff_report(split, only_in_raw, only_in_processed):\n",
    "    \"\"\"Save detailed diff report to file\"\"\"\n",
    "    \n",
    "    output_dir = Path(\"results/data_analysis\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    report_path = output_dir / f\"diff_{split}.json\"\n",
    "    \n",
    "    report = {\n",
    "        \"split\": split,\n",
    "        \"only_in_raw\": [\n",
    "            {\n",
    "                \"tokens\": list(tokens),\n",
    "                \"ner_tags\": list(tags)\n",
    "            }\n",
    "            for tokens, tags in only_in_raw\n",
    "        ],\n",
    "        \"only_in_processed\": [\n",
    "            {\n",
    "                \"tokens\": list(tokens),\n",
    "                \"ner_tags\": list(tags)\n",
    "            }\n",
    "            for tokens, tags in only_in_processed\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✓ Detailed report saved to: {report_path}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    raw_dir = r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\"\n",
    "    processed_dir = r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\"\n",
    "    \n",
    "    compare_datasets(raw_dir, processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c906db6",
   "metadata": {},
   "source": [
    "## Compare LR, Data, and CRF in BERT baseline (12 exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe12b4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TABLE 1: LEARNING RATE COMPARISON (Average across CRF and Data types)\n",
      "================================================================================\n",
      "          Avg Best Val F1  Std Best Val F1  Avg Test F1  Std Test F1  Count\n",
      "LR Value                                                                   \n",
      "1e-5             0.739247         0.004191     0.659698     0.002501      4\n",
      "2e-5             0.734300         0.003383     0.660347     0.006639      4\n",
      "5e-5             0.731520         0.002223     0.662997     0.007341      4\n",
      "\n",
      "================================================================================\n",
      "TABLE 2: DATA TYPE COMPARISON (Average across LR and CRF)\n",
      "================================================================================\n",
      "           Avg Best Val F1  Std Best Val F1  Avg Test F1  Std Test F1  Count\n",
      "Data Type                                                                   \n",
      "processed         0.735036         0.004994     0.662379     0.006556      6\n",
      "raw               0.735009         0.004465     0.659649     0.004469      6\n",
      "\n",
      "================================================================================\n",
      "DETAILED BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "All experiments:\n",
      "data_type   lr crf_type  best_val_f1  test_f1\n",
      "processed 1e-5      crf     0.733820 0.658730\n",
      "processed 1e-5    nocrf     0.743655 0.658456\n",
      "processed 2e-5      crf     0.737944 0.664477\n",
      "processed 2e-5    nocrf     0.733185 0.667214\n",
      "processed 5e-5      crf     0.730013 0.653804\n",
      "processed 5e-5    nocrf     0.731599 0.671593\n",
      "      raw 1e-5      crf     0.741037 0.663435\n",
      "      raw 1e-5    nocrf     0.738476 0.658172\n",
      "      raw 2e-5      crf     0.735915 0.656787\n",
      "      raw 2e-5    nocrf     0.730154 0.652909\n",
      "      raw 5e-5      crf     0.729834 0.662050\n",
      "      raw 5e-5    nocrf     0.734635 0.664543\n",
      "\n",
      "Overall Statistics:\n",
      "  Mean Best Val F1: 0.7350\n",
      "  Mean Test F1: 0.6610\n",
      "  Best Val F1 Range: [0.7298, 0.7437]\n",
      "  Test F1 Range: [0.6529, 0.6716]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the baseline results\n",
    "summary_path = Path(r\"D:\\Dafa\\Project\\queryner-kd\\results\\baseline\\1-12-bert\\summary-1-12-bert-token-mean.json\")\n",
    "with open(summary_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Parse all experiments\n",
    "experiments = results['successful_experiments']\n",
    "\n",
    "# Extract data\n",
    "data = []\n",
    "for exp in experiments:\n",
    "    # Parse experiment name: format is \"teacher_{data_type}_{lr}_{crf_type}\"\n",
    "    parts = exp['exp_name'].split('_')\n",
    "    data_type = parts[1]  # 'processed' or 'raw'\n",
    "    lr = parts[2]  # '1e-5', '2e-5', '5e-5'\n",
    "    crf_type = parts[3]  # 'crf' or 'nocrf'\n",
    "    \n",
    "    data.append({\n",
    "        'data_type': data_type,\n",
    "        'lr': lr,\n",
    "        'crf_type': crf_type,\n",
    "        'best_val_f1': exp['best_val_f1'],\n",
    "        'test_f1': exp['test_f1']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLE 1: LEARNING RATE COMPARISON (Average across CRF and Data types)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group by learning rate\n",
    "lr_comparison = df.groupby('lr')[['best_val_f1', 'test_f1']].agg(['mean', 'std', 'count'])\n",
    "lr_summary = pd.DataFrame({\n",
    "    'LR Value': lr_comparison.index,\n",
    "    'Avg Best Val F1': lr_comparison['best_val_f1']['mean'].values,\n",
    "    'Std Best Val F1': lr_comparison['best_val_f1']['std'].values,\n",
    "    'Avg Test F1': lr_comparison['test_f1']['mean'].values,\n",
    "    'Std Test F1': lr_comparison['test_f1']['std'].values,\n",
    "    'Count': lr_comparison['best_val_f1']['count'].values\n",
    "}).set_index('LR Value')\n",
    "\n",
    "print(lr_summary.to_string())\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLE 2: DATA TYPE COMPARISON (Average across LR and CRF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group by data type\n",
    "data_comparison = df.groupby('data_type')[['best_val_f1', 'test_f1']].agg(['mean', 'std', 'count'])\n",
    "data_summary = pd.DataFrame({\n",
    "    'Data Type': data_comparison.index,\n",
    "    'Avg Best Val F1': data_comparison['best_val_f1']['mean'].values,\n",
    "    'Std Best Val F1': data_comparison['best_val_f1']['std'].values,\n",
    "    'Avg Test F1': data_comparison['test_f1']['mean'].values,\n",
    "    'Std Test F1': data_comparison['test_f1']['std'].values,\n",
    "    'Count': data_comparison['best_val_f1']['count'].values\n",
    "}).set_index('Data Type')\n",
    "\n",
    "print(data_summary.to_string())\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DETAILED BREAKDOWN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show all combinations\n",
    "print(\"\\nAll experiments:\")\n",
    "print(df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Overall Statistics:\")\n",
    "print(f\"  Mean Best Val F1: {df['best_val_f1'].mean():.4f}\")\n",
    "print(f\"  Mean Test F1: {df['test_f1'].mean():.4f}\")\n",
    "print(f\"  Best Val F1 Range: [{df['best_val_f1'].min():.4f}, {df['best_val_f1'].max():.4f}]\")\n",
    "print(f\"  Test F1 Range: [{df['test_f1'].min():.4f}, {df['test_f1'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32e440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
