{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b216feb6",
   "metadata": {},
   "source": [
    "## Distribution Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ac560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from collections import defaultdict\n",
    "# import random\n",
    "\n",
    "# # Terapkan random seed untuk hasil yang dapat direproduksi\n",
    "# random.seed(42)\n",
    "\n",
    "# # Muat dataset\n",
    "# dataset = load_dataset(\"bltlab/queryner\")\n",
    "# label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "# # --- 1. Konsolidasi Label BIO ---\n",
    "# # Fungsi untuk menggabungkan label B- dan I- menjadi satu entitas\n",
    "# consolidated_labels_map = {}\n",
    "# consolidated_label_names = []\n",
    "# for label in label_list:\n",
    "#     if label.startswith('B-'):\n",
    "#         entity_name = label[2:]\n",
    "#         consolidated_labels_map[label] = entity_name\n",
    "#         consolidated_label_names.append(entity_name)\n",
    "#     elif label.startswith('I-'):\n",
    "#         entity_name = label[2:]\n",
    "#         consolidated_labels_map[label] = entity_name\n",
    "#         consolidated_label_names.append(entity_name)\n",
    "#     else: # 'O' label\n",
    "#         consolidated_labels_map[label] = label\n",
    "#         consolidated_label_names.append(label)\n",
    "# consolidated_label_names = sorted(list(set(consolidated_label_names)))\n",
    "\n",
    "# # Daftar label yang terpengaruh\n",
    "# labels_to_plot = ['condition', 'quantity', 'price', 'origin', 'time', 'product_number']\n",
    "\n",
    "# # Fungsi untuk menghitung entitas di split dan mengkonsolidasikan\n",
    "# def count_and_consolidate(split_data):\n",
    "#     counts = defaultdict(int)\n",
    "#     for example in split_data:\n",
    "#         for tag_id in example['ner_tags']:\n",
    "#             original_label = label_list[tag_id]\n",
    "#             consolidated_label = consolidated_labels_map[original_label]\n",
    "#             counts[consolidated_label] += 1\n",
    "#     return counts\n",
    "\n",
    "# # Fungsi untuk menghitung data dan persentase\n",
    "# def calculate_distribution(counts, labels_to_check):\n",
    "#     plot_data = defaultdict(dict)\n",
    "#     total_counts = {}\n",
    "\n",
    "#     for label in labels_to_check:\n",
    "#         total_counts[label] = sum(counts[split][label] for split in ['train', 'validation', 'test'])\n",
    "\n",
    "#     for label in labels_to_check:\n",
    "#         total = total_counts[label]\n",
    "#         if total > 0:\n",
    "#             for split in ['train', 'validation', 'test']:\n",
    "#                 count = counts[split][label]\n",
    "#                 percentage = (count / total) * 100 if total > 0 else 0\n",
    "#                 plot_data[label][split] = {'percentage': round(percentage, 2), 'count': count}\n",
    "#         else:\n",
    "#             for split in ['train', 'validation', 'test']:\n",
    "#                 plot_data[label][split] = {'percentage': 0, 'count': 0}\n",
    "\n",
    "#     return plot_data\n",
    "\n",
    "# # --- Tahap Awal: Hitung Distribusi Sebelum Penyesuaian ---\n",
    "# print(\"Menghitung distribusi entitas sebelum penyesuaian...\")\n",
    "# initial_counts = {\n",
    "#     'train': count_and_consolidate(dataset['train']),\n",
    "#     'validation': count_and_consolidate(dataset['validation']),\n",
    "#     'test': count_and_consolidate(dataset['test'])\n",
    "# }\n",
    "# initial_distribution = calculate_distribution(initial_counts, labels_to_plot)\n",
    "\n",
    "# # --- Tahap Penyesuaian Distribusi Entitas ---\n",
    "# print(\"\\nMelakukan penyesuaian distribusi entitas...\")\n",
    "# rebalanced_dataset = {split: list(dataset[split]) for split in dataset.keys()}\n",
    "\n",
    "# def move_samples(source_split_name, dest_split_name, label_to_move, num_to_move):\n",
    "#     global rebalanced_dataset\n",
    "    \n",
    "#     source_split = rebalanced_dataset[source_split_name]\n",
    "#     dest_split = rebalanced_dataset[dest_split_name]\n",
    "    \n",
    "#     potential_indices = [i for i, example in enumerate(source_split) if any(label_to_move in label_list[tag_id] for tag_id in example['ner_tags'])]\n",
    "    \n",
    "#     if len(potential_indices) >= num_to_move:\n",
    "#         indices_to_remove = random.sample(potential_indices, num_to_move)\n",
    "#     else:\n",
    "#         indices_to_remove = potential_indices\n",
    "    \n",
    "#     if len(indices_to_remove) > 0:\n",
    "#         samples_to_move = [source_split[i] for i in indices_to_remove]\n",
    "#         for i in sorted(indices_to_remove, reverse=True):\n",
    "#             del source_split[i]\n",
    "#         dest_split.extend(samples_to_move)\n",
    "\n",
    "# # Terapkan penyesuaian yang dijelaskan:\n",
    "# move_samples('train', 'test', 'condition', 15)\n",
    "# move_samples('test', 'train', 'quantity', 5)\n",
    "# move_samples('train', 'validation', 'price', 3)\n",
    "# move_samples('train', 'test', 'price', 2)\n",
    "# move_samples('test', 'validation', 'origin', 3)\n",
    "# move_samples('train', 'test', 'time', 3)\n",
    "# move_samples('test', 'validation', 'product_number', 3)\n",
    "\n",
    "# # --- Tahap Akhir: Hitung ulang distribusi setelah penyesuaian ---\n",
    "# print(\"\\nPenyesuaian selesai. Menghitung ulang distribusi entitas...\")\n",
    "# final_counts = {\n",
    "#     'train': count_and_consolidate(rebalanced_dataset['train']),\n",
    "#     'validation': count_and_consolidate(rebalanced_dataset['validation']),\n",
    "#     'test': count_and_consolidate(rebalanced_dataset['test'])\n",
    "# }\n",
    "# final_distribution = calculate_distribution(final_counts, labels_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0f2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# os.makedirs('data/raw', exist_ok=True)\n",
    "# os.makedirs('data/processed', exist_ok=True)\n",
    "# os.makedirs('data/distribution', exist_ok=True)\n",
    "\n",
    "# # Save original dataset to raw folder\n",
    "# print(\"\\nSaving original dataset...\")\n",
    "# for split in dataset.keys():\n",
    "#     dataset[split].to_json(f'data/raw/{split}.json')\n",
    "# print(\"Original dataset saved to data/raw/\")\n",
    "\n",
    "# # Save rebalanced dataset to processed folder\n",
    "# print(\"\\nSaving processed dataset...\")\n",
    "# for split, data in rebalanced_dataset.items():\n",
    "#     # Convert list of examples to a format that can be saved\n",
    "#     processed_data = {\n",
    "#         'examples': data\n",
    "#     }\n",
    "#     with open(f'data/processed/{split}.json', 'w', encoding='utf-8') as f:\n",
    "#         json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "# print(\"Processed dataset saved to data/processed/\")\n",
    "\n",
    "# # Save distributions to distribution folder\n",
    "# print(\"\\nSaving distribution data...\")\n",
    "# distribution_data = {\n",
    "#     'initial_distribution': initial_distribution,\n",
    "#     'final_distribution': final_distribution\n",
    "# }\n",
    "# with open('data/distribution/label_distributions.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(distribution_data, f, ensure_ascii=False, indent=2)\n",
    "# print(\"Distribution data saved to data/distribution/label_distributions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8799b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def reformat_jsonl_file(file_path):\n",
    "    examples = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data = json.loads(line)\n",
    "                examples.append(data)\n",
    "    \n",
    "    # Create output structure\n",
    "    output_data = {\"examples\": examples}\n",
    "    \n",
    "    # Overwrite the original file\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Reformatted {file_path}\")\n",
    "    print(f\"  Total examples: {len(examples)}\")\n",
    "\n",
    "\n",
    "# Usage examples:\n",
    "if __name__ == \"__main__\":\n",
    "    # Single file\n",
    "    reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\")\n",
    "    reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\")\n",
    "    reformat_jsonl_file(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\")\n",
    "    \n",
    "    # Or batch process all files\n",
    "    # raw_dir = Path(r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\")\n",
    "    # for json_file in raw_dir.glob(\"*.json\"):\n",
    "    #     reformat_jsonl_file(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76da3c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522181e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2b0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, label_pad_id=-100, max_length=128):\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = json.load(f)[\"examples\"]\n",
    "        self.data = raw\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_pad_id = label_pad_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx][\"tokens\"]\n",
    "        ner_tags = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # buat encoding untuk tokens \n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # align labels dengan tokens yang sudah diencoding (jadi kepotong2 sesuai tokenization)\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(self.label_pad_id)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(ner_tags[word_idx])\n",
    "            else:\n",
    "                aligned_labels.append(self.label_pad_id)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(aligned_labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a7c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_info(model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    id2label = config.id2label\n",
    "    label2id = config.label2id\n",
    "    num_labels = config.num_labels\n",
    "\n",
    "    label_info = {\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "        \"num_labels\": num_labels\n",
    "    }\n",
    "\n",
    "    return label_info\n",
    "\n",
    "def create_dataloaders(\n",
    "        train_path, val_path, test_path,\n",
    "        model_name,\n",
    "        batch_size=32,\n",
    "        max_length=128\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = NERDataset(train_path, tokenizer, max_length=max_length)\n",
    "    val_dataset = NERDataset(val_path, tokenizer, max_length=max_length)\n",
    "    test_dataset = NERDataset(test_path, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34497ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDafa\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mqueryner-kd\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDafa\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mqueryner-kd\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mvalidation.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDafa\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mqueryner-kd\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m label_info \u001b[38;5;241m=\u001b[39m load_label_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbltlab/queryner-augmented-data-bert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[1;34m(train_path, val_path, test_path, model_name, batch_size, max_length)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloaders\u001b[39m(\n\u001b[0;32m     16\u001b[0m         train_path, val_path, test_path,\n\u001b[0;32m     17\u001b[0m         model_name,\n\u001b[0;32m     18\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     19\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     20\u001b[0m ):\n\u001b[0;32m     21\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 23\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNERDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m NERDataset(val_path, tokenizer, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m     25\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m NERDataset(test_path, tokenizer, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mNERDataset.__init__\u001b[1;34m(self, data_path, tokenizer, label_pad_id, max_length)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_path, tokenizer, label_pad_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m raw\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 77)"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\",\n",
    "    val_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\",\n",
    "    test_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    batch_size=16,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "label_info = load_label_info(\"bltlab/queryner-augmented-data-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412972f",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1336c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c0d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFOutputLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "        self.crf = CRF(num_tags=num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, outputs, labels=None, mask=None):\n",
    "        emissions = self.fc(outputs)\n",
    "\n",
    "        if labels is not None:\n",
    "            # CRF requires first token to be valid, so we create a modified mask\n",
    "            # that ensures first token is always included\n",
    "            if mask is None:\n",
    "                mask = torch.ones_like(labels, dtype=torch.bool)\n",
    "            else:\n",
    "                mask = mask.bool()\n",
    "            \n",
    "            # Ensure first position is always valid for CRF\n",
    "            mask[:, 0] = True\n",
    "            \n",
    "            # Replace -100 with 0 (dummy label) to avoid index issues\n",
    "            labels_crf = labels.clone()\n",
    "            labels_crf[labels == -100] = 0\n",
    "            \n",
    "            # Calculate loss\n",
    "            log_likelihood = self.crf(emissions, tags=labels_crf, mask=mask, reduction=\"mean\")\n",
    "            loss = -log_likelihood\n",
    "            return {\"logits\": emissions, \"loss\": loss}\n",
    "        else:\n",
    "            if mask is None:\n",
    "                mask = torch.ones(outputs.shape[:2], dtype=torch.bool, device=outputs.device)\n",
    "            pred = self.crf.decode(emissions, mask=mask.bool())\n",
    "            return {\"logits\": emissions, \"pred\": pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0860ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNERModel(nn.Module):\n",
    "    def __init__(self, num_labels, use_crf=False):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_crf = use_crf\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        raise NotImplementedError(\"Forward method must be implemented in subclass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c94df923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryNERTeacher(BaseNERModel):\n",
    "    def __init__(self, model_name, label_info, use_crf=False):\n",
    "        super().__init__(num_labels=label_info[\"num_labels\"], use_crf=use_crf)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            id2label=label_info[\"id2label\"],\n",
    "            label2id=label_info[\"label2id\"]\n",
    "        )\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(self.config.hidden_size, self.config.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21a7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTStudent(BaseNERModel):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", label_info=None, use_crf=False):\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = label_info[\"num_labels\"]\n",
    "        super().__init__(num_labels=self.num_labels, use_crf=self.use_crf)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            id2label=label_info[\"id2label\"],\n",
    "            label2id=label_info[\"label2id\"]\n",
    "        )\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(self.config.hidden_size, self.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6510b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBertStudent(BaseNERModel):\n",
    "    def __init__(self, model_name=\"huawei-noah/TinyBERT_General_4L_312D\", label_info=None, use_crf=False):\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = label_info[\"num_labels\"]\n",
    "        super().__init__(num_labels=self.num_labels, use_crf=self.use_crf)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            id2label=label_info[\"id2label\"],\n",
    "            label2id=label_info[\"label2id\"]\n",
    "        )\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(self.config.hidden_size, self.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a01e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMStudent(BaseNERModel):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_labels, \n",
    "            use_crf=False,\n",
    "            model_name_for_vocab = 'bert-base-uncased',\n",
    "            emb_dim = 300,\n",
    "            lstm_hidden = 300,\n",
    "            label_info = None,\n",
    "            pad_token_id = 0\n",
    "        ):\n",
    "        super().__init__(num_labels, use_crf)\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name_for_vocab)\n",
    "        vocab_size = config.vocab_size\n",
    "        pad_token_id = config.pad_token_id\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token_id)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(lstm_hidden * 2, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(hidden_dim=lstm_hidden * 2, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        emb = self.embedding(input_ids)\n",
    "        emb = self.dropout(emb)\n",
    "        outputs, _ = self.lstm(emb)\n",
    "        sequence_output = outputs\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0af48f",
   "metadata": {},
   "source": [
    "## Knowledge Distillation Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f00f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3f6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/training/kd_trainer.py\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    return F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "def kl_divergence_loss(student_logits, teacher_logits, temperature):\n",
    "    p_teacher = F.log_softmax(teacher_logits / temperature, dim=-1)\n",
    "    p_student = F.softmax(student_logits / temperature, dim=-1)\n",
    "    loss = F.kl_div(p_teacher, p_student, reduction='batchmean')\n",
    "    loss = loss * (temperature ** 2)\n",
    "    return loss\n",
    "\n",
    "def kl_divergence_loss_masked(student_logits, teacher_logits, temperature, mask=None, eps=1e-12):\n",
    "    T = float(temperature)\n",
    "\n",
    "    student_log_prob = F.log_softmax(student_logits / T, dim=-1)   # (B, L, C)\n",
    "    teacher_prob = F.softmax(teacher_logits / T, dim=-1)           # (B, L, C)\n",
    "\n",
    "    kl_elem = F.kl_div(student_log_prob, teacher_prob, reduction='none')  # (B, L, C)\n",
    "\n",
    "    kl_token = kl_elem.sum(dim=-1)  # (B, L)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.bool()\n",
    "        valid_sum = mask.float().sum()\n",
    "        if valid_sum.item() == 0:\n",
    "            return torch.tensor(0.0, device=student_logits.device)\n",
    "        kl_sum = (kl_token * mask.float()).sum()\n",
    "        return (kl_sum / valid_sum) * (T * T)\n",
    "    else:\n",
    "        return kl_token.mean() * (T * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a4ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/training/kd_trainer.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def kl_divergence_loss_masked(student_logits, teacher_logits, temperature, mask=None, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute masked KL divergence loss for knowledge distillation.\n",
    "    \n",
    "    Args:\n",
    "        student_logits: (B, L, C) logits from student\n",
    "        teacher_logits: (B, L, C) logits from teacher\n",
    "        temperature: Temperature for softening distributions\n",
    "        mask: (B, L) attention mask (1 for valid tokens)\n",
    "        eps: Small constant for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    T = float(temperature)\n",
    "\n",
    "    student_log_prob = F.log_softmax(student_logits / T, dim=-1)   # (B, L, C)\n",
    "    teacher_prob = F.softmax(teacher_logits / T, dim=-1)           # (B, L, C)\n",
    "\n",
    "    kl_elem = F.kl_div(student_log_prob, teacher_prob, reduction='none')  # (B, L, C)\n",
    "    kl_token = kl_elem.sum(dim=-1)  # (B, L)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.bool()\n",
    "        valid_sum = mask.float().sum()\n",
    "        if valid_sum.item() == 0:\n",
    "            return torch.tensor(0.0, device=student_logits.device)\n",
    "        kl_sum = (kl_token * mask.float()).sum()\n",
    "        return (kl_sum / valid_sum) * (T * T)\n",
    "    else:\n",
    "        return kl_token.mean() * (T * T)\n",
    "\n",
    "\n",
    "def _to_tensor_preds(preds, batch_size, seq_len, device):\n",
    "    \"\"\"\n",
    "    Convert CRF decode output (list[list[int]] or list of tensors) into a tensor\n",
    "    of shape (batch_size, seq_len) padded with 0s. Caller must mask invalid tokens.\n",
    "    \"\"\"\n",
    "    pred_tensor = torch.zeros((batch_size, seq_len), dtype=torch.long, device=device)\n",
    "    for i, p in enumerate(preds):\n",
    "        if isinstance(p, torch.Tensor):\n",
    "            p = p.tolist()\n",
    "        L = len(p)\n",
    "        if L > 0:\n",
    "            pred_tensor[i, :L] = torch.tensor(p, dtype=torch.long, device=device)\n",
    "    return pred_tensor\n",
    "\n",
    "\n",
    "def _safe_get_pred_tensor(output, batch_size, seq_len, device):\n",
    "    \"\"\"\n",
    "    Return a (batch, seq_len) tensor of predictions from model output.\n",
    "    Handles:\n",
    "      - output[\"pred\"] is a tensor (batch, seq_len)\n",
    "      - output[\"pred\"] is a list of lists (per-seq predicted label ids)\n",
    "      - output has no \"pred\" (use logits.argmax)\n",
    "    \"\"\"\n",
    "    if \"pred\" in output:\n",
    "        pred = output[\"pred\"]\n",
    "        if isinstance(pred, torch.Tensor):\n",
    "            return pred.to(device)\n",
    "        else:\n",
    "            # assume list of lists\n",
    "            return _to_tensor_preds(pred, batch_size, seq_len, device)\n",
    "    elif \"logits\" in output:\n",
    "        return output[\"logits\"].argmax(dim=-1).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"No 'pred' or 'logits' in model output to produce predictions.\")\n",
    "\n",
    "\n",
    "def _accumulate_confusion_counts(preds_flat, labels_flat):\n",
    "    \"\"\"\n",
    "    Compute per-class TP, predicted_counts, actual_counts using vectors.\n",
    "    preds_flat and labels_flat are 1D torch.Long tensors on CPU or device.\n",
    "    Returns (tp_sum, pred_sum, actual_sum) and also total_tp, total_pred, total_actual per class sums.\n",
    "    \"\"\"\n",
    "    if preds_flat.numel() == 0:\n",
    "        return 0, 0, 0, None  # no valid tokens in this batch\n",
    "\n",
    "    max_label = int(max(int(preds_flat.max().item()), int(labels_flat.max().item())))\n",
    "    num_classes = max_label + 1\n",
    "\n",
    "    # compute per-class counts\n",
    "    tp_per_class = torch.zeros(num_classes, dtype=torch.long, device=preds_flat.device)\n",
    "    pred_per_class = torch.zeros(num_classes, dtype=torch.long, device=preds_flat.device)\n",
    "    actual_per_class = torch.zeros(num_classes, dtype=torch.long, device=preds_flat.device)\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        pred_mask = preds_flat == c\n",
    "        lab_mask = labels_flat == c\n",
    "        tp_per_class[c] = int((pred_mask & lab_mask).sum().item())\n",
    "        pred_per_class[c] = int(pred_mask.sum().item())\n",
    "        actual_per_class[c] = int(lab_mask.sum().item())\n",
    "\n",
    "    tp_sum = int(tp_per_class.sum().item())\n",
    "    pred_sum = int(pred_per_class.sum().item())\n",
    "    actual_sum = int(actual_per_class.sum().item())\n",
    "\n",
    "    return tp_sum, pred_sum, actual_sum, (tp_per_class.cpu().numpy(), pred_per_class.cpu().numpy(), actual_per_class.cpu().numpy())\n",
    "\n",
    "\n",
    "def _batch_metrics(pred_tensor, label_tensor, attention_mask):\n",
    "    \"\"\"\n",
    "    pred_tensor: (B, L)\n",
    "    label_tensor: (B, L) with -100 for ignored positions\n",
    "    attention_mask: (B, L) with 1 for valid tokens\n",
    "    Returns TP, predicted_count, actual_count (ints)\n",
    "    \"\"\"\n",
    "    mask = attention_mask.bool()\n",
    "    # also ensure labels not equal to -100 in valid positions\n",
    "    valid = mask & (label_tensor != -100)\n",
    "    if valid.sum().item() == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    preds_flat = pred_tensor[valid].view(-1)\n",
    "    labels_flat = label_tensor[valid].view(-1)\n",
    "\n",
    "    tp_sum, pred_sum, actual_sum, _ = _accumulate_confusion_counts(preds_flat, labels_flat)\n",
    "    return tp_sum, pred_sum, actual_sum\n",
    "\n",
    "\n",
    "def _final_metrics(tp_sum, pred_sum, actual_sum):\n",
    "    \"\"\"\n",
    "    Compute micro precision, recall, f1 from aggregated counts.\n",
    "    \"\"\"\n",
    "    precision = tp_sum / pred_sum if pred_sum > 0 else 0.0\n",
    "    recall = tp_sum / actual_sum if actual_sum > 0 else 0.0\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "class KDTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for both baseline (fine-tuning) and knowledge distillation.\n",
    "    \n",
    "    For baseline:\n",
    "        teacher_model=None, alpha=0, beta=1\n",
    "    \n",
    "    For KD:\n",
    "        teacher_model=<trained_model>, alpha=0.5, beta=0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model,\n",
    "        student_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler=None,\n",
    "        device=\"cuda\",\n",
    "        alpha=0.5,\n",
    "        beta=0.5,\n",
    "        temperature=2.0,\n",
    "        scheduler_type=\"plateau\"  # \"plateau\", \"cosine\", \"step\", or None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            teacher_model: Teacher model (None for baseline)\n",
    "            student_model: Student model to train\n",
    "            train_loader: Training DataLoader\n",
    "            val_loader: Validation DataLoader\n",
    "            optimizer: Optimizer for student model\n",
    "            scheduler: Learning rate scheduler (optional)\n",
    "            device: Device to use\n",
    "            alpha: Weight for KD loss (0 for baseline)\n",
    "            beta: Weight for student loss (1 for baseline)\n",
    "            temperature: Temperature for KD\n",
    "            scheduler_type: Type of scheduler for proper step() call\n",
    "        \"\"\"\n",
    "        self.student = student_model.to(device)\n",
    "        self.teacher = None  # ← FIX: Initialize to None\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_type = scheduler_type\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.T = temperature\n",
    "\n",
    "        # Setup teacher if provided\n",
    "        if teacher_model is not None:\n",
    "            self.teacher = teacher_model.to(device)\n",
    "            self.teacher.eval()  # Set to eval mode\n",
    "            for p in self.teacher.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        # Validate configuration\n",
    "        if self.alpha > 0 and self.teacher is None:\n",
    "            raise ValueError(\"alpha > 0 requires a teacher model!\")\n",
    "        \n",
    "        # Print training mode\n",
    "        mode = \"BASELINE\" if self.teacher is None else \"KNOWLEDGE DISTILLATION\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Mode: {mode}\")\n",
    "        print(f\"Alpha (KD loss weight): {self.alpha}\")\n",
    "        print(f\"Beta (Student loss weight): {self.beta}\")\n",
    "        print(f\"Temperature: {self.T}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    def compute_losses(self, batch):\n",
    "        \"\"\"\n",
    "        Compute losses for one batch.\n",
    "        \n",
    "        Returns:\n",
    "            loss_total: Combined loss\n",
    "            loss_kd: KD loss (0 if no teacher)\n",
    "            loss_student: Student task loss\n",
    "            pred_tensor: Predictions for metrics\n",
    "            labels: Ground truth labels\n",
    "            attention_mask: Attention mask\n",
    "        \"\"\"\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # Get teacher logits if needed\n",
    "        teacher_logits = None\n",
    "        if self.alpha > 0 and self.teacher is not None:\n",
    "            with torch.no_grad():\n",
    "                self.teacher.eval()\n",
    "                teacher_out = self.teacher(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                teacher_logits = teacher_out[\"logits\"]\n",
    "\n",
    "        # Get student outputs\n",
    "        student_out = self.student(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "        student_logits = student_out[\"logits\"]\n",
    "\n",
    "        # Compute KD loss if teacher provided\n",
    "        if teacher_logits is not None:\n",
    "            loss_kd = kl_divergence_loss_masked(\n",
    "                student_logits, \n",
    "                teacher_logits, \n",
    "                self.T, \n",
    "                mask=attention_mask\n",
    "            )\n",
    "        else:\n",
    "            loss_kd = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        # Get student task loss\n",
    "        loss_student = student_out.get(\"loss\", torch.tensor(0.0, device=self.device))\n",
    "\n",
    "        # Combined loss\n",
    "        loss_total = self.alpha * loss_kd + self.beta * loss_student\n",
    "\n",
    "        # Get predictions for metrics\n",
    "        pred_tensor = _safe_get_pred_tensor(student_out, batch_size, seq_len, self.device)\n",
    "\n",
    "        return loss_total, loss_kd, loss_student, pred_tensor, labels, attention_mask\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.student.train()\n",
    "        total_loss, total_kd, total_stu = 0.0, 0.0, 0.0\n",
    "        tp_acc, pred_acc, actual_acc = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            loss_total, loss_kd, loss_student, pred_tensor, labels, attention_mask = \\\n",
    "                self.compute_losses(batch)\n",
    "            \n",
    "            loss_total.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += float(loss_total.item())\n",
    "            total_kd += float(loss_kd.item())\n",
    "            total_stu += float(loss_student.item()) if isinstance(loss_student, torch.Tensor) else float(loss_student)\n",
    "\n",
    "            tp, pred_count, actual_count = _batch_metrics(pred_tensor, labels, attention_mask)\n",
    "            tp_acc += tp\n",
    "            pred_acc += pred_count\n",
    "            actual_acc += actual_count\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        avg_kd = total_kd / len(self.train_loader)\n",
    "        avg_stu = total_stu / len(self.train_loader)\n",
    "\n",
    "        # Update scheduler if provided\n",
    "        if self.scheduler:\n",
    "            if self.scheduler_type == \"plateau\":\n",
    "                self.scheduler.step(avg_loss)\n",
    "            else:\n",
    "                # For cosine, step, etc. that don't need metrics\n",
    "                self.scheduler.step()\n",
    "\n",
    "        precision, recall, f1 = _final_metrics(tp_acc, pred_acc, actual_acc)\n",
    "        return avg_loss, avg_kd, avg_stu, precision, recall, f1\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate on validation set.\"\"\"\n",
    "        self.student.eval()\n",
    "        total_loss, total_kd, total_stu = 0.0, 0.0, 0.0\n",
    "        tp_acc, pred_acc, actual_acc = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                loss_total, loss_kd, loss_student, pred_tensor, labels, attention_mask = \\\n",
    "                    self.compute_losses(batch)\n",
    "                \n",
    "                total_loss += float(loss_total.item())\n",
    "                total_kd += float(loss_kd.item())\n",
    "                total_stu += float(loss_student.item()) if isinstance(loss_student, torch.Tensor) else float(loss_student)\n",
    "\n",
    "                tp, pred_count, actual_count = _batch_metrics(pred_tensor, labels, attention_mask)\n",
    "                tp_acc += tp\n",
    "                pred_acc += pred_count\n",
    "                actual_acc += actual_count\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_kd = total_kd / len(self.val_loader)\n",
    "        avg_stu = total_stu / len(self.val_loader)\n",
    "\n",
    "        precision, recall, f1 = _final_metrics(tp_acc, pred_acc, actual_acc)\n",
    "        return avg_loss, avg_kd, avg_stu, precision, recall, f1\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Train for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            num_epochs: Number of epochs to train\n",
    "            \n",
    "        Returns:\n",
    "            history: Dictionary containing training history\n",
    "        \"\"\"\n",
    "        history = {\n",
    "            \"train_loss\": [], \"val_loss\": [],\n",
    "            \"train_kd\": [], \"val_kd\": [],\n",
    "            \"train_stu\": [], \"val_stu\": [],\n",
    "            \"train_precision\": [], \"train_recall\": [], \"train_f1\": [],\n",
    "            \"val_precision\": [], \"val_recall\": [], \"val_f1\": []\n",
    "        }\n",
    "\n",
    "        best_val_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"EPOCH {epoch}/{num_epochs}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            train_loss, train_kd, train_stu, train_prec, train_rec, train_f1 = self.train_epoch()\n",
    "            val_loss, val_kd, val_stu, val_prec, val_rec, val_f1 = self.validate()\n",
    "\n",
    "            print(f\"\\nTrain Loss: {train_loss:.4f} (KD: {train_kd:.4f}, Student: {train_stu:.4f})\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} (KD: {val_kd:.4f}, Student: {val_stu:.4f})\")\n",
    "            print(f\"\\nTrain Metrics -> P: {train_prec:.4f}, R: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Metrics   -> P: {val_prec:.4f}, R: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Track best validation F1\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                print(f\"✓ New best Val F1: {best_val_f1:.4f}\")\n",
    "\n",
    "            # Store history\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_kd\"].append(train_kd)\n",
    "            history[\"train_stu\"].append(train_stu)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_kd\"].append(val_kd)\n",
    "            history[\"val_stu\"].append(val_stu)\n",
    "\n",
    "            history[\"train_precision\"].append(train_prec)\n",
    "            history[\"train_recall\"].append(train_rec)\n",
    "            history[\"train_f1\"].append(train_f1)\n",
    "            history[\"val_precision\"].append(val_prec)\n",
    "            history[\"val_recall\"].append(val_rec)\n",
    "            history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Complete!\")\n",
    "        print(f\"Best Val F1: {best_val_f1:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff72159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.AdamW(student_bilstm.parameters(), lr=2e-5)\n",
    "\n",
    "# trainer = KDTrainer(\n",
    "#     teacher_model=None,\n",
    "#     student_model=student_bilstm,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     optimizer=optimizer,\n",
    "#     alpha=0,\n",
    "#     beta=1,\n",
    "#     temperature=2.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae8d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535dfe1a",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "190381f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "BASELINE TRAINING: 48 EXPERIMENTS\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Epochs: 10\n",
      "Batch Size: 16\n",
      "Max Length: 128\n",
      "Running experiments 1 to 3\n",
      "================================================================================\n",
      "\n",
      "Loading label information...\n",
      "Number of labels: 35\n",
      "\n",
      "Total experiments to run: 3\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Experiment 1/48: distilbert_processed_2e-5_crf\n",
      "================================================================================\n",
      "Model: distilbert\n",
      "Dataset: processed\n",
      "Learning Rate: 2e-5\n",
      "CRF: True\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Instantiating model: DistilBERTStudent...\n",
      "\n",
      "============================================================\n",
      "Training Mode: BASELINE\n",
      "Alpha (KD loss weight): 0.0\n",
      "Beta (Student loss weight): 1.0\n",
      "Temperature: 2.0\n",
      "============================================================\n",
      "\n",
      "Training for 10 epochs...\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25ddec619924dfd9f45e1fba8492add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8171f499d12460c906d702f02ca5a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 6.7917 (KD: 0.0000, Student: 6.7917)\n",
      "Val Loss:   3.9571 (KD: 0.0000, Student: 3.9571)\n",
      "\n",
      "Train Metrics -> P: 0.5023, R: 0.5023, F1: 0.5023\n",
      "Val Metrics   -> P: 0.6758, R: 0.6758, F1: 0.6758\n",
      "✓ New best Val F1: 0.6758\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cf2b633b7043b7b612b8ca430b20ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0878b8c69c3e497db698897998d1f66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.7094 (KD: 0.0000, Student: 3.7094)\n",
      "Val Loss:   3.4726 (KD: 0.0000, Student: 3.4726)\n",
      "\n",
      "Train Metrics -> P: 0.6989, R: 0.6989, F1: 0.6989\n",
      "Val Metrics   -> P: 0.7002, R: 0.7002, F1: 0.7002\n",
      "✓ New best Val F1: 0.7002\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094fc379b9314356a8d636988d30eceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e538b6c541478c8817adf501afe139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.7780 (KD: 0.0000, Student: 2.7780)\n",
      "Val Loss:   3.3376 (KD: 0.0000, Student: 3.3376)\n",
      "\n",
      "Train Metrics -> P: 0.7704, R: 0.7704, F1: 0.7704\n",
      "Val Metrics   -> P: 0.7154, R: 0.7154, F1: 0.7154\n",
      "✓ New best Val F1: 0.7154\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534251ef984f4bb0ad94804af6bf40bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67f1518580945218a375ba025672054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0389 (KD: 0.0000, Student: 2.0389)\n",
      "Val Loss:   3.4147 (KD: 0.0000, Student: 3.4147)\n",
      "\n",
      "Train Metrics -> P: 0.8311, R: 0.8311, F1: 0.8311\n",
      "Val Metrics   -> P: 0.7227, R: 0.7227, F1: 0.7227\n",
      "✓ New best Val F1: 0.7227\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fff29bad1d472f867b2bc5661baccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a5adbd04d341ceb05df479fe5fc33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4585 (KD: 0.0000, Student: 1.4585)\n",
      "Val Loss:   3.6460 (KD: 0.0000, Student: 3.6460)\n",
      "\n",
      "Train Metrics -> P: 0.8815, R: 0.8815, F1: 0.8815\n",
      "Val Metrics   -> P: 0.7192, R: 0.7192, F1: 0.7192\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb7cf976c7b400aa27f1aeabd5e7dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d47b2fca964c5d836abd41464a0a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0675 (KD: 0.0000, Student: 1.0675)\n",
      "Val Loss:   3.8530 (KD: 0.0000, Student: 3.8530)\n",
      "\n",
      "Train Metrics -> P: 0.9137, R: 0.9137, F1: 0.9137\n",
      "Val Metrics   -> P: 0.7167, R: 0.7167, F1: 0.7167\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9124cee38b4cdb865b4b8c68bf084d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b43e6fd3904aa2bb391ee4a4b90ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.7862 (KD: 0.0000, Student: 0.7862)\n",
      "Val Loss:   4.1551 (KD: 0.0000, Student: 4.1551)\n",
      "\n",
      "Train Metrics -> P: 0.9371, R: 0.9371, F1: 0.9371\n",
      "Val Metrics   -> P: 0.7176, R: 0.7176, F1: 0.7176\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9c0a5e4c144b4ea20490caf4b65ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893e7da3d58a49b1b829536804b43bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.5532 (KD: 0.0000, Student: 0.5532)\n",
      "Val Loss:   4.4942 (KD: 0.0000, Student: 4.4942)\n",
      "\n",
      "Train Metrics -> P: 0.9571, R: 0.9571, F1: 0.9571\n",
      "Val Metrics   -> P: 0.7192, R: 0.7192, F1: 0.7192\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e3cc33ce9d4294bdfe36903e307ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fb85847c9540bca116692c0de50dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.4157 (KD: 0.0000, Student: 0.4157)\n",
      "Val Loss:   4.6711 (KD: 0.0000, Student: 4.6711)\n",
      "\n",
      "Train Metrics -> P: 0.9690, R: 0.9690, F1: 0.9690\n",
      "Val Metrics   -> P: 0.7195, R: 0.7195, F1: 0.7195\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4635451c39914079bf5f85edbce77a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbb895738e449d7a61ce3a7c87ee922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.3285 (KD: 0.0000, Student: 0.3285)\n",
      "Val Loss:   4.9584 (KD: 0.0000, Student: 4.9584)\n",
      "\n",
      "Train Metrics -> P: 0.9749, R: 0.9749, F1: 0.9749\n",
      "Val Metrics   -> P: 0.7230, R: 0.7230, F1: 0.7230\n",
      "✓ New best Val F1: 0.7230\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "Best Val F1: 0.7230\n",
      "============================================================\n",
      "\n",
      "✓ Results saved to: results\\baseline\\json\\distilbert_processed_2e-5_crf.json\n",
      "\n",
      "✓ Experiment 1 completed successfully!\n",
      "Final Val F1: 0.7230\n",
      "Best Val F1: 0.7230\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Experiment 2/48: distilbert_processed_2e-5_nocrf\n",
      "================================================================================\n",
      "Model: distilbert\n",
      "Dataset: processed\n",
      "Learning Rate: 2e-5\n",
      "CRF: False\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Instantiating model: DistilBERTStudent...\n",
      "\n",
      "============================================================\n",
      "Training Mode: BASELINE\n",
      "Alpha (KD loss weight): 0.0\n",
      "Beta (Student loss weight): 1.0\n",
      "Temperature: 2.0\n",
      "============================================================\n",
      "\n",
      "Training for 10 epochs...\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a334a75fb9494b2ab1f06744c543e2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21e27f68600490ba9f23e844f26901b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.5580 (KD: 0.0000, Student: 1.5580)\n",
      "Val Loss:   1.0513 (KD: 0.0000, Student: 1.0513)\n",
      "\n",
      "Train Metrics -> P: 0.5634, R: 0.5634, F1: 0.5634\n",
      "Val Metrics   -> P: 0.6935, R: 0.6935, F1: 0.6935\n",
      "✓ New best Val F1: 0.6935\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100c1912a3b04257b955e1cf3beb795b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9b33c827444fd884a9bb3541b6bdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.9466 (KD: 0.0000, Student: 0.9466)\n",
      "Val Loss:   0.9525 (KD: 0.0000, Student: 0.9525)\n",
      "\n",
      "Train Metrics -> P: 0.7181, R: 0.7181, F1: 0.7181\n",
      "Val Metrics   -> P: 0.7154, R: 0.7154, F1: 0.7154\n",
      "✓ New best Val F1: 0.7154\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bfac0f300d4d9eb5318b61882df5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ec8fce12f143c6aa4e39601c3a135a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.7088 (KD: 0.0000, Student: 0.7088)\n",
      "Val Loss:   0.9260 (KD: 0.0000, Student: 0.9260)\n",
      "\n",
      "Train Metrics -> P: 0.7898, R: 0.7898, F1: 0.7898\n",
      "Val Metrics   -> P: 0.7253, R: 0.7253, F1: 0.7253\n",
      "✓ New best Val F1: 0.7253\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe87382fc264919bbdc1d40f1ed371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42563f4623744dd998c04a4e8b52ae8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.5182 (KD: 0.0000, Student: 0.5182)\n",
      "Val Loss:   0.9465 (KD: 0.0000, Student: 0.9465)\n",
      "\n",
      "Train Metrics -> P: 0.8446, R: 0.8446, F1: 0.8446\n",
      "Val Metrics   -> P: 0.7300, R: 0.7300, F1: 0.7300\n",
      "✓ New best Val F1: 0.7300\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1e429a840e4ad0a01408146732cca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 328\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Run all baselines\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# You can also run in batches:\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# run_all_baselines(device=device, start_from=1, end_at=12)  # First 12\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# run_all_baselines(device=device, start_from=13, end_at=24)  # Next 12\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# run_all_baselines(device=device, start_from=25, end_at=36)  # Next 12\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# run_all_baselines(device=device, start_from=37, end_at=48)  # Last 12\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[43mrun_all_baselines\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m    335\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 246\u001b[0m, in \u001b[0;36mrun_all_baselines\u001b[1;34m(device, num_epochs, batch_size, max_length, start_from, end_at)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(experiments, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m         history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# Store summary\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         final_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mmax\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    262\u001b[0m         }\n",
      "Cell \u001b[1;32mIn[17], line 168\u001b[0m, in \u001b[0;36mtrain_single_experiment\u001b[1;34m(exp, label_info, device, num_epochs, batch_size, max_length)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 168\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m    171\u001b[0m save_experiment_results(exp, history)\n",
      "Cell \u001b[1;32mIn[14], line 361\u001b[0m, in \u001b[0;36mKDTrainer.train\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 361\u001b[0m train_loss, train_kd, train_stu, train_prec, train_rec, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m val_loss, val_kd, val_stu, val_prec, val_rec, val_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (KD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_kd\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Student: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_stu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 280\u001b[0m, in \u001b[0;36mKDTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    279\u001b[0m     loss_total, loss_kd, loss_student, pred_tensor, labels, attention_mask \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     loss_total\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[14], line 257\u001b[0m, in \u001b[0;36mKDTrainer.compute_losses\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    250\u001b[0m     loss_kd \u001b[38;5;241m=\u001b[39m kl_divergence_loss_masked(\n\u001b[0;32m    251\u001b[0m         student_logits, \n\u001b[0;32m    252\u001b[0m         teacher_logits, \n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT, \n\u001b[0;32m    254\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     loss_kd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Get student task loss\u001b[39;00m\n\u001b[0;32m    260\u001b[0m loss_student \u001b[38;5;241m=\u001b[39m student_out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Baseline Training Script for NER Models\n",
    "Trains 4 models × 2 datasets × 3 learning rates × 2 CRF settings = 48 experiments\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Import your existing modules (assumed to be available)\n",
    "# from your_module import (\n",
    "#     NERDataset, create_dataloaders, load_label_info,\n",
    "#     QueryNERTeacher, DistilBERTStudent, TinyBertStudent, BiLSTMStudent,\n",
    "#     KDTrainer\n",
    "# )\n",
    "\n",
    "\n",
    "def create_experiment_config():\n",
    "    \"\"\"Generate all 48 experiment configurations\"\"\"\n",
    "    \n",
    "    # Configuration space\n",
    "    models = [\n",
    "        # (\"teacher\", \"QueryNERTeacher\", \"bltlab/queryner-augmented-data-bert-base-uncased\"),\n",
    "        (\"distilbert\", \"DistilBERTStudent\", \"distilbert-base-uncased\"),\n",
    "        (\"tinybert\", \"TinyBertStudent\", \"huawei-noah/TinyBERT_General_4L_312D\"),\n",
    "        (\"bilstm\", \"BiLSTMStudent\", \"bert-base-uncased\")\n",
    "    ]\n",
    "    \n",
    "    datasets = [\n",
    "        (\"processed\", {\n",
    "            \"train\": r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\train.json\",\n",
    "            \"val\": r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\validation.json\",\n",
    "            \"test\": r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\test.json\"\n",
    "        }),\n",
    "        (\"raw\", {\n",
    "            \"train\": r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\train.json\",\n",
    "            \"val\": r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\validation.json\",\n",
    "            \"test\": r\"D:\\Dafa\\Project\\queryner-kd\\data\\raw\\test.json\"\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    learning_rates = [\n",
    "        (2e-5, \"2e-5\"),   # Conservative, stable\n",
    "        (5e-5, \"5e-5\"),   # Balanced speed/stability\n",
    "        (1e-4, \"1e-4\")    # Faster convergence\n",
    "    ]\n",
    "    \n",
    "    crf_settings = [\n",
    "        (True, \"crf\"),\n",
    "        (False, \"nocrf\")\n",
    "    ]\n",
    "    \n",
    "    # Generate all combinations\n",
    "    experiments = []\n",
    "    exp_id = 1\n",
    "    \n",
    "    for model_name, model_class, model_path in models:\n",
    "        for data_name, data_paths in datasets:\n",
    "            for lr_value, lr_name in learning_rates:\n",
    "                for use_crf, crf_name in crf_settings:\n",
    "                    exp = {\n",
    "                        \"id\": exp_id,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"model_class\": model_class,\n",
    "                        \"model_path\": model_path,\n",
    "                        \"data_name\": data_name,\n",
    "                        \"data_paths\": data_paths,\n",
    "                        \"learning_rate\": lr_value,\n",
    "                        \"lr_name\": lr_name,\n",
    "                        \"use_crf\": use_crf,\n",
    "                        \"crf_name\": crf_name,\n",
    "                        \"exp_name\": f\"{model_name}_{data_name}_{lr_name}_{crf_name}\"\n",
    "                    }\n",
    "                    experiments.append(exp)\n",
    "                    exp_id += 1\n",
    "    \n",
    "    return experiments\n",
    "\n",
    "\n",
    "def instantiate_model(model_class, model_path, label_info, use_crf, device):\n",
    "    \"\"\"Instantiate the correct model based on class name\"\"\"\n",
    "    \n",
    "    if model_class == \"QueryNERTeacher\":\n",
    "        model = QueryNERTeacher(\n",
    "            model_name=model_path,\n",
    "            label_info=label_info,\n",
    "            use_crf=use_crf\n",
    "        )\n",
    "    elif model_class == \"DistilBERTStudent\":\n",
    "        model = DistilBERTStudent(\n",
    "            model_name=model_path,\n",
    "            label_info=label_info,\n",
    "            use_crf=use_crf\n",
    "        )\n",
    "    elif model_class == \"TinyBertStudent\":\n",
    "        model = TinyBertStudent(\n",
    "            model_name=model_path,\n",
    "            label_info=label_info,\n",
    "            use_crf=use_crf\n",
    "        )\n",
    "    elif model_class == \"BiLSTMStudent\":\n",
    "        model = BiLSTMStudent(\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            use_crf=use_crf,\n",
    "            model_name_for_vocab=model_path,\n",
    "            label_info=label_info\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model class: {model_class}\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def train_single_experiment(exp, label_info, device, num_epochs=10, batch_size=16, max_length=128):\n",
    "    \"\"\"Train a single experiment configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Experiment {exp['id']}/48: {exp['exp_name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Model: {exp['model_name']}\")\n",
    "    print(f\"Dataset: {exp['data_name']}\")\n",
    "    print(f\"Learning Rate: {exp['lr_name']}\")\n",
    "    print(f\"CRF: {exp['use_crf']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    print(\"Loading data...\")\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_path=exp['data_paths']['train'],\n",
    "        val_path=exp['data_paths']['val'],\n",
    "        test_path=exp['data_paths']['test'],\n",
    "        model_name=\"bert-base-uncased\",  # tokenizer\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Instantiate model\n",
    "    print(f\"Instantiating model: {exp['model_class']}...\")\n",
    "    model = instantiate_model(\n",
    "        model_class=exp['model_class'],\n",
    "        model_path=exp['model_path'],\n",
    "        label_info=label_info,\n",
    "        use_crf=exp['use_crf'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=exp['learning_rate'])\n",
    "    \n",
    "    # Create trainer (using KD trainer with alpha=0, beta=1 for baseline)\n",
    "    trainer = KDTrainer(\n",
    "        teacher_model=None,  # No teacher for baseline\n",
    "        student_model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        alpha=0.0,  # No KD loss\n",
    "        beta=1.0,   # Only student loss\n",
    "        temperature=2.0  # Not used when alpha=0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    history = trainer.train(num_epochs=num_epochs)\n",
    "    \n",
    "    # Save results\n",
    "    save_experiment_results(exp, history)\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, trainer, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def save_experiment_results(exp, history):\n",
    "    \"\"\"Save experiment results in organized structure\"\"\"\n",
    "    \n",
    "    # Create directory structure\n",
    "    base_dir = Path(\"results/baseline\")\n",
    "    json_dir = base_dir / \"json\"\n",
    "    img_dir = base_dir / \"img\"\n",
    "    \n",
    "    json_dir.mkdir(parents=True, exist_ok=True)\n",
    "    img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save history as JSON\n",
    "    json_path = json_dir / f\"{exp['exp_name']}.json\"\n",
    "    \n",
    "    result_data = {\n",
    "        \"experiment\": exp,\n",
    "        \"history\": history,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(result_data, f, indent=4)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {json_path}\")\n",
    "\n",
    "\n",
    "def run_all_baselines(\n",
    "    device=\"cuda\",\n",
    "    num_epochs=10,\n",
    "    batch_size=16,\n",
    "    max_length=128,\n",
    "    start_from=1,\n",
    "    end_at=48\n",
    "):\n",
    "    \"\"\"Run all 48 baseline experiments\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BASELINE TRAINING: 48 EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Max Length: {max_length}\")\n",
    "    print(f\"Running experiments {start_from} to {end_at}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load label info (use teacher model config)\n",
    "    print(\"Loading label information...\")\n",
    "    label_info = load_label_info(\"bltlab/queryner-augmented-data-bert-base-uncased\")\n",
    "    print(f\"Number of labels: {label_info['num_labels']}\")\n",
    "    \n",
    "    # Generate all experiment configs\n",
    "    experiments = create_experiment_config()\n",
    "    \n",
    "    # Filter experiments based on start_from and end_at\n",
    "    experiments = [exp for exp in experiments if start_from <= exp['id'] <= end_at]\n",
    "    \n",
    "    print(f\"\\nTotal experiments to run: {len(experiments)}\\n\")\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    failed_experiments = []\n",
    "    \n",
    "    # Run each experiment\n",
    "    for i, exp in enumerate(experiments, 1):\n",
    "        try:\n",
    "            history = train_single_experiment(\n",
    "                exp=exp,\n",
    "                label_info=label_info,\n",
    "                device=device,\n",
    "                num_epochs=num_epochs,\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            # Store summary\n",
    "            final_metrics = {\n",
    "                \"exp_name\": exp['exp_name'],\n",
    "                \"val_f1\": history['val_f1'][-1],\n",
    "                \"val_precision\": history['val_precision'][-1],\n",
    "                \"val_recall\": history['val_recall'][-1],\n",
    "                \"best_val_f1\": max(history['val_f1'])\n",
    "            }\n",
    "            all_results.append(final_metrics)\n",
    "            \n",
    "            print(f\"\\n✓ Experiment {exp['id']} completed successfully!\")\n",
    "            print(f\"Final Val F1: {final_metrics['val_f1']:.4f}\")\n",
    "            print(f\"Best Val F1: {final_metrics['best_val_f1']:.4f}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Experiment {exp['id']} FAILED!\")\n",
    "            print(f\"Error: {str(e)}\\n\")\n",
    "            failed_experiments.append({\n",
    "                \"exp_id\": exp['id'],\n",
    "                \"exp_name\": exp['exp_name'],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    # Save summary\n",
    "    save_summary(all_results, failed_experiments)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Successful: {len(all_results)}\")\n",
    "    print(f\"Failed: {len(failed_experiments)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "def save_summary(all_results, failed_experiments):\n",
    "    \"\"\"Save summary of all experiments\"\"\"\n",
    "    \n",
    "    summary_dir = Path(\"results/baseline\")\n",
    "    summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save results summary\n",
    "    summary_path = summary_dir / \"summary.json\"\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"successful_experiments\": all_results,\n",
    "            \"failed_experiments\": failed_experiments,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    print(f\"\\n✓ Summary saved to: {summary_path}\")\n",
    "    \n",
    "    # Print top performing models\n",
    "    if all_results:\n",
    "        print(\"\\nTop 5 Models by Best Val F1:\")\n",
    "        sorted_results = sorted(all_results, key=lambda x: x['best_val_f1'], reverse=True)\n",
    "        for i, result in enumerate(sorted_results[:5], 1):\n",
    "            print(f\"{i}. {result['exp_name']}: F1={result['best_val_f1']:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Check device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Run all baselines\n",
    "    # You can also run in batches:\n",
    "    # run_all_baselines(device=device, start_from=1, end_at=12)  # First 12\n",
    "    # run_all_baselines(device=device, start_from=13, end_at=24)  # Next 12\n",
    "    # run_all_baselines(device=device, start_from=25, end_at=36)  # Next 12\n",
    "    # run_all_baselines(device=device, start_from=37, end_at=48)  # Last 12\n",
    "    \n",
    "    run_all_baselines(\n",
    "        device=device,\n",
    "        num_epochs=10,\n",
    "        batch_size=16,\n",
    "        max_length=128,\n",
    "        start_from=1,\n",
    "        end_at=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ce995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
