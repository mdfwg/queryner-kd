{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da811d97",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578e3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0936a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinUpgradationTime(req1, t1, req2, t2):\n",
    "    # Find minimum time T where both services have completed their required upgrades\n",
    "    # Service 1 needs t1 upgrade opportunities (time % req1 != 0)\n",
    "    # Service 2 needs t2 upgrade opportunities (time % req2 != 0)\n",
    "    \n",
    "    count1 = 0  # upgrade opportunities for service 1\n",
    "    count2 = 0  # upgrade opportunities for service 2\n",
    "    time = 1\n",
    "    \n",
    "    while count1 < t1 or count2 < t2:\n",
    "        if time % req1 != 0:\n",
    "            count1 += 1\n",
    "        if time % req2 != 0:\n",
    "            count2 += 1\n",
    "        \n",
    "        if count1 >= t1 and count2 >= t2:\n",
    "            return time\n",
    "        \n",
    "        time += 1\n",
    "    \n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9161e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMinUpgradationTime(3, 2, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3024f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual trace for test 1: (3, 2, 4, 1)\n",
      "Service 1 can upgrade when time % 3 != 0\n",
      "Service 2 can upgrade when time % 4 != 0\n",
      "\n",
      "Service 1 first 5 upgrade times: [1, 2, 4, 5, 7]\n",
      "Service 2 first 5 upgrade times: [1, 2, 3, 5, 6]\n",
      "For t1=2, t2=1: we need arr1_full[0:2]=[1, 2] and arr2_full[0:1]=[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's trace through test 1 manually to understand the pattern\n",
    "# Test 1: req1=3, t1=2, req2=4, t2=1, expected=3\n",
    "\n",
    "print(\"Manual trace for test 1: (3, 2, 4, 1)\")\n",
    "print(\"Service 1 can upgrade when time % 3 != 0\")\n",
    "print(\"Service 2 can upgrade when time % 4 != 0\")\n",
    "print()\n",
    "\n",
    "# Generate more times to understand the pattern\n",
    "arr1_full = []\n",
    "time = 1\n",
    "while len(arr1_full) < 5:\n",
    "    if time % 3 != 0:\n",
    "        arr1_full.append(time)\n",
    "    time += 1\n",
    "\n",
    "arr2_full = []\n",
    "time = 1\n",
    "while len(arr2_full) < 5:\n",
    "    if time % 4 != 0:\n",
    "        arr2_full.append(time)\n",
    "    time += 1\n",
    "\n",
    "print(f\"Service 1 first 5 upgrade times: {arr1_full}\")\n",
    "print(f\"Service 2 first 5 upgrade times: {arr2_full}\")\n",
    "print(f\"For t1=2, t2=1: we need arr1_full[0:2]={arr1_full[0:2]} and arr2_full[0:1]={arr2_full[0:1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfac4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed trace for (3, 2, 4, 1):\n",
      "Finding min T where req1=3 has >=2 chances AND req2=4 has >=1 chance\n",
      "\n",
      "Time 1: S1_count=1, S2_count=1\n",
      "Time 2: S1_count=2, S2_count=2\n",
      "✓ Both requirements met at time 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Detailed trace for test 1\n",
    "print(\"\\nDetailed trace for (3, 2, 4, 1):\")\n",
    "print(\"Finding min T where req1=3 has >=2 chances AND req2=4 has >=1 chance\\n\")\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for t in range(1, 10):\n",
    "    if t % 3 != 0:\n",
    "        count1 += 1\n",
    "    if t % 4 != 0:\n",
    "        count2 += 1\n",
    "    print(f\"Time {t}: S1_count={count1}, S2_count={count2}\")\n",
    "    if count1 >= 2 and count2 >= 1:\n",
    "        print(f\"✓ Both requirements met at time {t}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24d91c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trace for (2, 1, 3, 3), expected 4:\n",
      "Time 1: S1_count=1, S2_count=1\n",
      "Time 2: S1_count=1, S2_count=2\n",
      "Time 3: S1_count=2, S2_count=2\n",
      "Time 4: S1_count=2, S2_count=3\n",
      "✓ Found at time 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trace test 2: (2, 1, 3, 3)\n",
    "print(\"\\nTrace for (2, 1, 3, 3), expected 4:\")\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for t in range(1, 10):\n",
    "    if t % 2 != 0:\n",
    "        count1 += 1\n",
    "    if t % 3 != 0:\n",
    "        count2 += 1\n",
    "    print(f\"Time {t}: S1_count={count1}, S2_count={count2}\")\n",
    "    if count1 >= 1 and count2 >= 3:\n",
    "        print(f\"✓ Found at time {t}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff8da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20489828",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9a3b264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7823\n",
      "{'tokens': ['teeth', 'whitening', 'sensitive', 'teeth'], 'ner_tags': [9, 10, 17, 18]}\n",
      "{'tokens': ['white', 'duvet', 'cover', 'queen'], 'ner_tags': [3, 9, 10, 1]}\n"
     ]
    }
   ],
   "source": [
    "path = r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\train.json\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)[\"examples\"]\n",
    "\n",
    "print(len(raw))\n",
    "for item in raw[:2]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a72521f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teeth', 'whitening', 'sensitive', 'teeth']\n",
      "[9, 10, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "tokens = raw[0][\"tokens\"]\n",
    "ner_tags = raw[0][\"ner_tags\"]\n",
    "print(tokens)\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "23907683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6a29ffae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4091, 2317, 5582, 7591, 4091,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "670961cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 2, 3, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "word_ids = encodings.word_ids(batch_index=0)\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "07a27736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 4091, 2317, 5582, 7591, 4091,  102,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e50ff6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 9, 10, -100, 17, 18, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = []\n",
    "previous_word_idx = None\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None:\n",
    "        aligned_labels.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        aligned_labels.append(ner_tags[word_idx])\n",
    "    else:\n",
    "        aligned_labels.append(-100)\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "print(aligned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "12cc6fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 101, 4091, 2317, 5582, 7591, 4091,  102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([-100,    9,   10, -100,   17,   18, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])}\n"
     ]
    }
   ],
   "source": [
    "item = {\n",
    "    \"input_ids\": encodings[\"input_ids\"].squeeze(0),\n",
    "    \"attention_mask\": encodings[\"attention_mask\"].squeeze(0),\n",
    "    \"labels\": torch.tensor(aligned_labels, dtype=torch.long)\n",
    "}\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a5f84955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, label_pad_id=-100, max_length=128):\n",
    "        ## khusus untuk data json\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = json.load(f)[\"examples\"]\n",
    "        self.data = raw\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_pad_id = label_pad_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx][\"tokens\"]\n",
    "        ner_tags = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # buat encoding untuk tokens \n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # align labels dengan tokens yang sudah diencoding (jadi kepotong2 sesuai tokenization)\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(self.label_pad_id)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(ner_tags[word_idx])\n",
    "            else:\n",
    "                aligned_labels.append(self.label_pad_id)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(aligned_labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ff168235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_info(model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    id2label = config.id2label\n",
    "    label2id = config.label2id\n",
    "    num_labels = config.num_labels\n",
    "\n",
    "    label_info = {\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "        \"num_labels\": num_labels\n",
    "    }\n",
    "\n",
    "    return label_info\n",
    "\n",
    "def create_dataloaders(\n",
    "        train_path, val_path, test_path,\n",
    "        model_name,\n",
    "        batch_size=16,\n",
    "        max_length=128\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = NERDataset(train_path, tokenizer, max_length=max_length)\n",
    "    val_dataset = NERDataset(val_path, tokenizer, max_length=max_length)\n",
    "    test_dataset = NERDataset(test_path, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85a2e0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NERDataset at 0x1e09721a5a0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = NERDataset(\n",
    "    data_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\train.json\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ce727dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e094517950>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3b83493b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  8441,  3345,  ...,     0,     0,     0],\n",
       "         [  101, 18144, 25309,  ...,     0,     0,     0],\n",
       "         [  101, 13653,  9242,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 11584,  3514,  ...,     0,     0,     0],\n",
       "         [  101,  3680, 11527,  ...,     0,     0,     0],\n",
       "         [  101, 16215,  9307,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[-100,   31,   32,  ..., -100, -100, -100],\n",
       "         [-100,    9,   10,  ..., -100, -100, -100],\n",
       "         [-100,   11,   31,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,   17,    9,  ..., -100, -100, -100],\n",
       "         [-100,   11,   12,  ..., -100, -100, -100],\n",
       "         [-100,   11, -100,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddc8f6",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "88e8ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2af932ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, label_pad_id=-100, max_length=128):\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = json.load(f)[\"examples\"]\n",
    "        self.data = raw\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_pad_id = label_pad_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx][\"tokens\"]\n",
    "        ner_tags = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # buat encoding untuk tokens \n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # align labels dengan tokens yang sudah diencoding (jadi kepotong2 sesuai tokenization)\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(self.label_pad_id)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(ner_tags[word_idx])\n",
    "            else:\n",
    "                aligned_labels.append(self.label_pad_id)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(aligned_labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "db94990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_info(model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    id2label = config.id2label\n",
    "    label2id = config.label2id\n",
    "    num_labels = config.num_labels\n",
    "\n",
    "    label_info = {\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id,\n",
    "        \"num_labels\": num_labels\n",
    "    }\n",
    "\n",
    "    return label_info\n",
    "\n",
    "def create_dataloaders(\n",
    "        train_path, val_path, test_path,\n",
    "        model_name,\n",
    "        batch_size=32,\n",
    "        max_length=128\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = NERDataset(train_path, tokenizer, max_length=max_length)\n",
    "    val_dataset = NERDataset(val_path, tokenizer, max_length=max_length)\n",
    "    test_dataset = NERDataset(test_path, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3f43c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\train.json\",\n",
    "    val_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\validation.json\",\n",
    "    test_path=r\"D:\\Dafa\\Project\\queryner-kd\\data\\processed\\test.json\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    batch_size=16,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "label_info = load_label_info(\"bltlab/queryner-augmented-data-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cf9cf707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2273,  2015,  ...,     0,     0,     0],\n",
       "         [  101, 21299,  9949,  ...,     0,     0,     0],\n",
       "         [  101, 12745, 19958,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 11608,  4524,  ...,     0,     0,     0],\n",
       "         [  101,  1046, 16558,  ...,     0,     0,     0],\n",
       "         [  101, 22894,  9541,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[-100,   13, -100,  ..., -100, -100, -100],\n",
       "         [-100,   11,   17,  ..., -100, -100, -100],\n",
       "         [-100,   17,    9,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,   11,    9,  ..., -100, -100, -100],\n",
       "         [-100,   11, -100,  ..., -100, -100, -100],\n",
       "         [-100,   11, -100,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6a14afc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id2label', 'label2id', 'num_labels'])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a2775909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "32576556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseNERModel\n",
    "label_info = label_info\n",
    "use_crf = False\n",
    "\n",
    "# QueryNER\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=label_info[\"num_labels\"],\n",
    "    id2label=label_info[\"id2label\"],\n",
    "    label2id=label_info[\"label2id\"]\n",
    ")\n",
    "\n",
    "bert = AutoModel.from_pretrained(model_name, config=config)\n",
    "dropout = nn.Dropout(0.1)\n",
    "classifier = nn.Linear(bert.config.hidden_size, bert.config.num_labels)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "632ca302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 768])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward\n",
    "outputs = bert(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "outputs.last_hidden_state.shape  # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4c86719a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 768])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_output = dropout(outputs.last_hidden_state)\n",
    "sequence_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c72e0",
   "metadata": {},
   "source": [
    "#### Use CRF False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8d0f372b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 35])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = classifier(sequence_output)  # (batch_size, seq_len, num_labels)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3f8f5835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6777, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(\n",
    "    logits.view(-1, label_info[\"num_labels\"]),\n",
    "    batch[\"labels\"].view(-1)\n",
    ")\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5662fed",
   "metadata": {},
   "source": [
    "#### Use CRF True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2f21e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.crf_output = CRFOutputLayer(self.config.hidden_size, self.num_labels)\n",
    "# result = self.crf_output(sequence_output, labels=labels, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "07e92bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "# input nya sequence_output, labels, mask\n",
    "fc = nn.Linear(bert.config.hidden_size, bert.config.num_labels)\n",
    "crf = CRF(label_info[\"num_labels\"], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "00fb7821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 35])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissions = fc(sequence_output)  # (batch_size, seq_len, num_labels)\n",
    "emissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "26de6248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bc2bf697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,   13, -100,  ..., -100, -100, -100],\n",
       "        [-100,   11,   17,  ..., -100, -100, -100],\n",
       "        [-100,   17,    9,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,   11,    9,  ..., -100, -100, -100],\n",
       "        [-100,   11, -100,  ..., -100, -100, -100],\n",
       "        [-100,   11, -100,  ..., -100, -100, -100]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "63ac6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = batch[\"attention_mask\"]\n",
    "mask = mask.bool()\n",
    "mask[:, 0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "06bb0e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 13,  0,  ...,  0,  0,  0],\n",
       "        [ 0, 11, 17,  ...,  0,  0,  0],\n",
       "        [ 0, 17,  9,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0, 11,  9,  ...,  0,  0,  0],\n",
       "        [ 0, 11,  0,  ...,  0,  0,  0],\n",
       "        [ 0, 11,  0,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_crf = batch[\"labels\"]\n",
    "labels_crf[batch[\"labels\"] == -100] = 0\n",
    "labels_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "763fdb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5357, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likehood = crf(emissions, batch[\"labels\"], mask=batch[\"attention_mask\"].bool(), reduction='token_mean')\n",
    "loss = -log_likehood\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95d4084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23, 7, 3, 25, 25, 29, 25, 6],\n",
       " [2, 7, 2, 1, 22, 3, 4],\n",
       " [23, 0, 0, 3],\n",
       " [23, 23, 32, 2, 32, 1, 1, 23, 21],\n",
       " [17, 2, 25, 1, 19, 14, 11, 25],\n",
       " [23, 20, 29, 33, 18, 1, 28, 29],\n",
       " [17, 32, 23, 14, 28, 6],\n",
       " [2, 20, 13, 9],\n",
       " [27, 6, 13, 26, 29, 27, 1, 29, 26],\n",
       " [17, 2, 27, 3, 13, 3, 29, 3],\n",
       " [23, 6, 28, 13, 1, 28],\n",
       " [25, 27, 13, 25, 29, 3],\n",
       " [30, 13, 23, 13, 21],\n",
       " [30, 11, 20, 6, 5],\n",
       " [30, 2, 11, 0, 28],\n",
       " [23, 2, 27, 23, 27, 23, 3]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = crf.decode(emissions, mask=batch[\"attention_mask\"].bool())\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9982c",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579796b",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "64ef0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b02c9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFOutputLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "        self.crf = CRF(num_tags=num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, outputs, labels=None, mask=None):\n",
    "        emissions = self.fc(outputs)\n",
    "\n",
    "        if labels is not None:\n",
    "            if mask is None:\n",
    "                mask = torch.ones_like(labels, dtype=torch.bool)\n",
    "            else:\n",
    "                mask = mask.bool()\n",
    "            \n",
    "            mask[:, 0] = True\n",
    "            \n",
    "            labels_crf = labels.clone()\n",
    "            labels_crf[labels == -100] = 0\n",
    "            \n",
    "            log_likelihood = self.crf(emissions, tags=labels_crf, mask=mask, reduction=\"token_mean\")\n",
    "            loss = -log_likelihood\n",
    "            return {\"logits\": emissions, \"loss\": loss}\n",
    "        else:\n",
    "            if mask is None:\n",
    "                mask = torch.ones(outputs.shape[:2], dtype=torch.bool, device=outputs.device)\n",
    "            pred = self.crf.decode(emissions, mask=mask.bool())\n",
    "            return {\"logits\": emissions, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "63263ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNERModel(nn.Module):\n",
    "    def __init__(self, num_labels, use_crf=False):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_crf = use_crf\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        raise NotImplementedError(\"Forward method must be implemented in subclass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6eee64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryNERTeacher(BaseNERModel):\n",
    "    def __init__(self, model_name, label_info, use_crf=False):\n",
    "        super().__init__(num_labels=label_info[\"num_labels\"], use_crf=use_crf)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            id2label=label_info[\"id2label\"],\n",
    "            label2id=label_info[\"label2id\"]\n",
    "        )\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(self.config.hidden_size, self.config.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ab335654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTStudent(BaseNERModel):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", label_info=None, use_crf=False):\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = label_info[\"num_labels\"]\n",
    "        super().__init__(num_labels=self.num_labels, use_crf=self.use_crf)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            id2label=label_info[\"id2label\"],\n",
    "            label2id=label_info[\"label2id\"]\n",
    "        )\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(self.config.hidden_size, self.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "53ef83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBertStudent(BaseNERModel):\n",
    "    def __init__(self, model_name=\"huawei-noah/TinyBERT_General_4L_312D\", label_info=None, use_crf=False):\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = label_info[\"num_labels\"]\n",
    "        super().__init__(num_labels=self.num_labels, use_crf=self.use_crf)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=label_info[\"num_labels\"],\n",
    "            id2label=label_info[\"id2label\"],\n",
    "            label2id=label_info[\"label2id\"]\n",
    "        )\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(self.config.hidden_size, self.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c788b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMStudent(BaseNERModel):\n",
    "    def __init__(\n",
    "            self,  \n",
    "            use_crf=False,\n",
    "            model_name_for_vocab = 'bert-base-uncased',\n",
    "            emb_dim = 300,\n",
    "            lstm_hidden = 300,\n",
    "            label_info = None,\n",
    "            pad_token_id = 0,\n",
    "            teacher_model = None\n",
    "        ):\n",
    "        self.use_crf = use_crf\n",
    "        self.num_labels = label_info[\"num_labels\"]\n",
    "        super().__init__(self.num_labels, use_crf)\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name_for_vocab)\n",
    "        vocab_size = config.vocab_size\n",
    "        pad_token_id = config.pad_token_id\n",
    "\n",
    "        if teacher_model is not None:\n",
    "            self.embedding = teacher_model.bert.embeddings.word_embeddings\n",
    "            for p in self.embedding.parameters():\n",
    "                p.requires_grad = False\n",
    "            emb_dim_eff = self.embedding.embedding_dim\n",
    "        else:\n",
    "            emb_dim_eff = emb_dim\n",
    "            self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token_id)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim_eff,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(lstm_hidden * 2, self.num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.crf_output = CRFOutputLayer(hidden_dim=lstm_hidden * 2, num_labels=self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        emb = self.embedding(input_ids)\n",
    "        emb = self.dropout(emb)\n",
    "        outputs, _ = self.lstm(emb)\n",
    "        sequence_output = outputs\n",
    "\n",
    "        if self.use_crf:\n",
    "            mask = attention_mask.bool()\n",
    "            result = self.crf_output(sequence_output, labels=labels, mask=mask)\n",
    "            return result\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output)\n",
    "            if labels is not None:\n",
    "                loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                return {\"logits\": logits, \"loss\": loss}\n",
    "            else:\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                return {\"logits\": logits, \"pred\": pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e375118",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3fe22ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6f815857",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = QueryNERTeacher(\"bert-base-uncased\", label_info, use_crf=False)\n",
    "student = DistilBERTStudent(label_info=label_info, use_crf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "99e552b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "16545f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([16, 128])\n",
      "attention_mask shape: torch.Size([16, 128])\n",
      "labels shape: torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "print(\"labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c0e0aba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128, 35])\n",
      "tensor([[[ 0.5233,  0.1475,  0.6589,  0.1023, -0.3250, -0.0386,  0.3790,\n",
      "          -0.1430,  0.4473,  0.1549, -0.5810,  0.0186, -0.6553,  0.1660,\n",
      "          -0.1445, -0.2970, -0.1980,  0.3350, -0.2192, -1.0898, -0.0928,\n",
      "          -0.1468,  0.1173, -0.2140, -0.8575,  0.0469,  0.2112,  0.1077,\n",
      "          -0.1394,  0.3507, -0.0735,  0.3273, -0.3279, -0.5097, -0.8232]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor(3.8230, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t_result = teacher(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "print(t_result[\"logits\"].shape)\n",
    "print(t_result[\"logits\"][:1, :1])\n",
    "print(t_result[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ad0608bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logits', 'loss'])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_result = student(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "s_result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4948a",
   "metadata": {},
   "source": [
    "#### KL Div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fcff0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "01883cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_teacher = F.log_softmax(t_result[\"logits\"]/T, dim=-1)\n",
    "# print(\"T2: \", p_teacher[:1, :1])\n",
    "p_student = F.softmax(s_result[\"logits\"] / T, dim=-1)\n",
    "# print(\"Student\", p_student[:1, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "766daa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3002, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.kl_div(p_teacher, p_student, reduction=\"batchmean\")\n",
    "loss = loss * (2**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684c74d",
   "metadata": {},
   "source": [
    "#### KL Div Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "30a145aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 35])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_elem = F.kl_div(p_teacher, p_student, reduction=\"none\")\n",
    "kl_elem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6100c5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_token = kl_elem.sum(dim=-1)\n",
    "# dim = -1 > the summation working on the last dimension (B,L,C) > sum for each class\n",
    "kl_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eac1f5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = batch[\"attention_mask\"]\n",
    "mask = mask.bool()\n",
    "mask.float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2edea69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(107.)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sum = mask.float().sum()\n",
    "valid_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b26da0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1099, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if valid_sum == 0:\n",
    "    print(torch.tensor(0.0, device=s_result[\"logits\"].device))\n",
    "kl_sum = (kl_token * mask.float()).sum()\n",
    "print((kl_sum/valid_sum)*(T*T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "73e23de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0648, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_token.mean()*(T*T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730aa08",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13939e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With T=0.5:\n",
      "tensor([[[0.0314, 0.0862, 0.0725, 0.0089, 0.0369, 0.0183, 0.0583, 0.0331,\n",
      "          0.0164, 0.0687]]], grad_fn=<SliceBackward0>)\n",
      "With T=1:\n",
      "tensor([[[0.0317, 0.0525, 0.0482, 0.0169, 0.0344, 0.0242, 0.0432, 0.0325,\n",
      "          0.0229, 0.0469]]], grad_fn=<SliceBackward0>)\n",
      "With T=2:\n",
      "tensor([[[0.0305, 0.0393, 0.0376, 0.0223, 0.0318, 0.0267, 0.0356, 0.0309,\n",
      "          0.0259, 0.0371]]], grad_fn=<SliceBackward0>)\n",
      "With T=4:\n",
      "tensor([[[0.0296, 0.0336, 0.0329, 0.0253, 0.0303, 0.0277, 0.0320, 0.0298,\n",
      "          0.0273, 0.0327]]], grad_fn=<SliceBackward0>)\n",
      "With T=8:\n",
      "tensor([[[0.0291, 0.0310, 0.0307, 0.0269, 0.0294, 0.0282, 0.0303, 0.0292,\n",
      "          0.0280, 0.0306]]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for T in [0.5, 1, 2, 4, 8]:\n",
    "    softmax_res = F.softmax(logits/T, dim=-1)\n",
    "    print(f\"With T={T}:\\n\" + str(softmax_res[:1, :1, :10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "F.kl_div()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f0d22",
   "metadata": {},
   "source": [
    "## CRF Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b60eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 5\n",
    "model = CRF(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7358b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n",
      "tensor([[[-0.9194, -0.4299, -2.3956,  0.7575,  0.5029],\n",
      "         [ 0.3142, -0.8432, -0.7544, -0.6260, -0.8528]],\n",
      "\n",
      "        [[-1.1581, -0.1062, -0.3376,  0.0401, -0.3961],\n",
      "         [ 1.1090, -1.3114, -0.1248, -0.4729,  0.4600]],\n",
      "\n",
      "        [[-0.5804,  0.7643,  0.8325, -0.6142, -1.6916],\n",
      "         [-0.8408, -0.5836,  0.6385, -1.1154,  1.0496]]])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 3\n",
    "batch_size = 2\n",
    "emissions = torch.randn(seq_length, batch_size, num_tags)\n",
    "print(emissions.shape)\n",
    "print(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7036e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = torch.tensor([\n",
    "    [0, 1], [2, 4], [3, 1]], dtype=torch.long\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-12.1175, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(emissions, tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
